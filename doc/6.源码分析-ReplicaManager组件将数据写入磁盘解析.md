[TOC]

# ReplicaManager副本管理组件

broker接收到消息数据后，会通过ReplicaManager写入到磁盘文件，并且同步给follower

接收到客户端发送消息的请求类型，broker通过KafkaApis下的handleProducerRequest方法处理

```java
...
//调用副本管理组件，将消息同步给所有 replica
replicaManager.appendMessages(
produceRequest.timeout.toLong,
produceRequest.acks,
internalTopicsAllowed,
authorizedMessagesPerPartition,
sendResponseCallback)
...
```

ReplicaManager初始化流程如下

```java
object ReplicaManager {
  val HighWatermarkFilename = "replication-offset-checkpoint"
  val IsrChangePropagationBlackOut = 5000L
  val IsrChangePropagationInterval = 60000L
}

class ReplicaManager(val config: KafkaConfig,
                     metrics: Metrics,
                     time: Time,
                     jTime: JTime,
                     val zkUtils: ZkUtils,
                     scheduler: Scheduler,
                     val logManager: LogManager,
                     val isShuttingDown: AtomicBoolean,
                     threadNamePrefix: Option[String] = None) extends Logging with KafkaMetricsGroup {
  /* epoch of the controller that last changed the leader */
  @volatile var controllerEpoch: Int = KafkaController.InitialControllerEpoch - 1
  private val localBrokerId = config.brokerId

  //保存了当前这台 broker 上存储的所有 partition
  private val allPartitions = new Pool[(String, Int), Partition](valueFactory = Some { case (t, p) =>
    new Partition(t, p, time, this)
  })
  private val replicaStateChangeLock = new Object
  //负责拉取replica的组件
  val replicaFetcherManager = new ReplicaFetcherManager(config, this, metrics, jTime, threadNamePrefix)

  //每个 leader 写入了一条消息，leader partition 的 LEO 会推进一位
  //但是必须等到所有的 follower 都同步了这条消息，partition 的 HW 才能整体推进一位
  //消费者只能读到 HW 高水位以下的消息
  private val highWatermarkCheckPointThreadStarted = new AtomicBoolean(false)
  val highWatermarkCheckpoints = config.logDirs.map(dir => (new File(dir).getAbsolutePath, new OffsetCheckpoint(new File(dir, ReplicaManager.HighWatermarkFilename)))).toMap
  private var hwThreadInitialized = false
  this.logIdent = "[Replica Manager on Broker " + localBrokerId + "]: "
  val stateChangeLogger = KafkaController.stateChangeLogger

  //isr 列表，leader 一定在列表中，follower 却不一定
  //必须是 follower 跟上了 leader 的数据同步，没有落后太多，才会维护在 isr 列表中
  //下面这些数据结构都是用来维护 isr 列表的
  private val isrChangeSet: mutable.Set[TopicAndPartition] = new mutable.HashSet[TopicAndPartition]()
  private val lastIsrChangeMs = new AtomicLong(System.currentTimeMillis())
  private val lastIsrPropagationMs = new AtomicLong(System.currentTimeMillis())

  //时间轮算法实现的延迟调度机制
  val delayedProducePurgatory = DelayedOperationPurgatory[DelayedProduce](
    purgatoryName = "Produce", config.brokerId, config.producerPurgatoryPurgeIntervalRequests)
  val delayedFetchPurgatory = DelayedOperationPurgatory[DelayedFetch](
    purgatoryName = "Fetch", config.brokerId, config.fetchPurgatoryPurgeIntervalRequests)

  //本地 leader 的数量
  val leaderCount = newGauge(
    "LeaderCount",
    new Gauge[Int] {
      def value = {
          getLeaderPartitions().size
      }
    }
  )

  //本地 partition 的数量
  val partitionCount = newGauge(
    "PartitionCount",
    new Gauge[Int] {
      def value = allPartitions.size
    }
  )

  //副本数量不充足的 partition
  val underReplicatedPartitions = newGauge(
    "UnderReplicatedPartitions",
    new Gauge[Int] {
      def value = underReplicatedPartitionCount()
    }
  )

  //isr 列表扩张和伸缩的速率
  val isrExpandRate = newMeter("IsrExpandsPerSec",  "expands", TimeUnit.SECONDS)
  val isrShrinkRate = newMeter("IsrShrinksPerSec",  "shrinks", TimeUnit.SECONDS)

  ...
```

# ReplicaManager启动维护ISR列表后台线程

在KafkaServer初始化的时候，会通过ReplicaManager启动后台定时任务isr-expiration，将replica在指定的时间内都没有追赶上leader LEO的follower从ISR中移除。需要注意的是，有follower从ISR移除后，需要重新更新leader的HW

```java
/* start replica manager */
replicaManager = new ReplicaManager(config, metrics, time, kafkaMetricsTime, zkUtils, kafkaScheduler, logManager,
  isShuttingDown)
//启动后台维护线程
replicaManager.startup()
```

启动后台定时任务线程

```java
def startup() {
    // start ISR expiration thread
    //后台线程检测是否需要将 replica 从 ISR 列表中移除，replica.lag.time.max.ms默认10000
    scheduler.schedule("isr-expiration", maybeShrinkIsr, period = config.replicaLagTimeMaxMs, unit = TimeUnit.MILLISECONDS)
    scheduler.schedule("isr-change-propagation", maybePropagateIsrChanges, period = 2500L, unit = TimeUnit.MILLISECONDS)
  }
```

计算哪些分区可以从ISR中移除

```java
private def maybeShrinkIsr(): Unit = {
    trace("Evaluating ISR list of partitions to see which replicas can be removed from the ISR")
    allPartitions.values.foreach(partition => partition.maybeShrinkIsr(config.replicaLagTimeMaxMs))
  }
```

```java
def maybeShrinkIsr(replicaMaxLagTimeMs: Long) {
    val leaderHWIncremented = inWriteLock(leaderIsrUpdateLock) {
      //如果 leader partition 在当前 broker
      leaderReplicaIfLocal() match {
        case Some(leaderReplica) =>
          //获取同步慢的 replica
          val outOfSyncReplicas = getOutOfSyncReplicas(leaderReplica, replicaMaxLagTimeMs)
          if(outOfSyncReplicas.size > 0) {
            //从ISR中移除同步慢的replica
            val newInSyncReplicas = inSyncReplicas -- outOfSyncReplicas
            assert(newInSyncReplicas.size > 0)
            info("Shrinking ISR for partition [%s,%d] from %s to %s".format(topic, partitionId,
              inSyncReplicas.map(_.brokerId).mkString(","), newInSyncReplicas.map(_.brokerId).mkString(",")))
            // update ISR in zk and in cache
            //更新新的ISR列表到zk和本地缓存
            updateIsr(newInSyncReplicas)
            // we may need to increment high watermark since ISR could be down to 1

            replicaManager.isrShrinkRate.mark()
            //可能需要更新 leader 的 HW
            maybeIncrementLeaderHW(leaderReplica)
          } else {
            false
          }

        case None => false // do nothing if no longer leader
      }
    }

    // some delayed operations may be unblocked after HW changed
    if (leaderHWIncremented)
      tryCompleteDelayedRequests()
  }
```

获取ISR列表中，从leader分区同步慢的replica

```java
def getOutOfSyncReplicas(leaderReplica: Replica, maxLagMs: Long): Set[Replica] = {
    /**
     * there are two cases that will be handled here -
     * 1. Stuck followers: If the leo of the replica hasn't been updated for maxLagMs ms,
     *                     the follower is stuck and should be removed from the ISR
     * 2. Slow followers: If the replica has not read up to the leo within the last maxLagMs ms,
     *                    then the follower is lagging and should be removed from the ISR
     * Both these cases are handled by checking the lastCaughtUpTimeMs which represents
     * the last time when the replica was fully caught up. If either of the above conditions
     * is violated, that replica is considered to be out of sync
     *
      * 对于一个 follower 而言，如果超过10s 都没有发起 fetch 请求，说明该 follower 被卡住
      * 1.follower 所在的机器 broker 宕机
      * 2.follower 所在的机器 broker full gc
      *
      * 在10s 内，follwer 都无法跟进到当前最新的 LEO，说明 follower 同步数据太慢
      * 1.kafka 部署的机器负载过高，导致网络负载高
      * 2.磁盘负载高，读写性能不佳
      * 3.full gc
     **/
    val leaderLogEndOffset = leaderReplica.logEndOffset

    //从 ISR 列表中排除 leader replica
    val candidateReplicas = inSyncReplicas - leaderReplica

    //判断最近的 fetch 时间是否超过指定
    val laggingReplicas = candidateReplicas.filter(r => (time.milliseconds - r.lastCaughtUpTimeMs) > maxLagMs)
    if(laggingReplicas.size > 0)
      debug("Lagging replicas for partition %s are %s".format(TopicAndPartition(topic, partitionId), laggingReplicas.map(_.brokerId).mkString(",")))

    laggingReplicas
  }
```

# 消息追加到本地磁盘文件

将消息发送给 leader partition，并且等待其复制给其他的 replica。callback函数将会在超时或者满足acks要求的情况下被触发调用。每个分区对应一个MessageSet，集合中的每条消息都有固定的格式回调函数的结构，也是每个分区对应一个PartitionResponse，响应中包含对一个分区磁盘文件的数据写入的结果

```java
def appendMessages(timeout: Long,
                     requiredAcks: Short,
                     internalTopicsAllowed: Boolean,
                     messagesPerPartition: Map[TopicPartition, MessageSet],
                     responseCallback: Map[TopicPartition, PartitionResponse] => Unit) {

    //检验ack必须是-1|1|0
    //1:leader 写入成功即可
    //-1/all:需要等待ISR集合中的所有副本都确认收到消息之后才能正确地收到响应的结果
    //0:不管写入是否成功直接返回
    if (isValidRequiredAcks(requiredAcks)) {
      val sTime = SystemTime.milliseconds
      //将消息写入本地的leader partition分区磁盘文件，并返回结果
      val localProduceResults = appendToLocalLog(internalTopicsAllowed, messagesPerPartition, requiredAcks)
      debug("Produce to local log in %d ms".format(SystemTime.milliseconds - sTime))

      //封装写入本地数据的结果
      val produceStatus = localProduceResults.map { case (topicPartition, result) =>
        topicPartition ->
                ProducePartitionStatus(
                  result.info.lastOffset + 1, // required offset
                  new PartitionResponse(result.errorCode, result.info.firstOffset, result.info.timestamp)) // response status
      }

      //判断是否需要等待其他replicate返回响应
      if (delayedRequestRequired(requiredAcks, messagesPerPartition, localProduceResults)) {
        // create delayed produce operation
        val produceMetadata = ProduceMetadata(requiredAcks, produceStatus)
        val delayedProduce = new DelayedProduce(timeout, produceMetadata, this, responseCallback)

        // create a list of (topic, partition) pairs to use as keys for this delayed produce operation
        val producerRequestKeys = messagesPerPartition.keys.map(new TopicPartitionOperationKey(_)).toSeq

        // try to complete the request immediately, otherwise put it into the purgatory
        // this is because while the delayed produce operation is being created, new
        // requests may arrive and hence make this operation completable.
        delayedProducePurgatory.tryCompleteElseWatch(delayedProduce, producerRequestKeys)

      } else {
        // we can respond immediately
        // 不需要等待其他replicate结果，直接回调并返回
        val produceResponseStatus = produceStatus.mapValues(status => status.responseStatus)
        responseCallback(produceResponseStatus)
      }
    } else {
      // If required.acks is outside accepted range, something is wrong with the client
      // Just return an error and don't handle the request at all
      //ack 异常，封装异常响应
      val responseStatus = messagesPerPartition.map {
        case (topicAndPartition, messageSet) =>
          (topicAndPartition -> new PartitionResponse(Errors.INVALID_REQUIRED_ACKS.code,
                                                      LogAppendInfo.UnknownLogAppendInfo.firstOffset,
                                                      Message.NoTimestamp))
      }
      responseCallback(responseStatus)
    }
  }
```

消息追加到本地磁盘文件

```java
private def appendToLocalLog(internalTopicsAllowed: Boolean,
                               messagesPerPartition: Map[TopicPartition, MessageSet],
                               requiredAcks: Short): Map[TopicPartition, LogAppendResult] = {
    trace("Append [%s] to local log ".format(messagesPerPartition))
    messagesPerPartition.map { case (topicPartition, messages) =>
      BrokerTopicStats.getBrokerTopicStats(topicPartition.topic).totalProduceRequestRate.mark()
      BrokerTopicStats.getBrokerAllTopicsStats().totalProduceRequestRate.mark()

      //内部 topic， 类似__consumer_offsets供内部使用
      // reject appending to internal topics if it is not allowed
      if (Topic.isInternal(topicPartition.topic) && !internalTopicsAllowed) {
        (topicPartition, LogAppendResult(
          LogAppendInfo.UnknownLogAppendInfo,
          Some(new InvalidTopicException("Cannot append to internal topic %s".format(topicPartition.topic)))))
      } else {
        try {
          //获取分区对应的Partition对象，当前broker保存了所有partition信息
          val partitionOpt = getPartition(topicPartition.topic, topicPartition.partition)
          val info = partitionOpt match {
            case Some(partition) =>
              //将需要写入该分区的数据写入磁盘文件
              partition.appendMessagesToLeader(messages.asInstanceOf[ByteBufferMessageSet], requiredAcks)
            case None => throw new UnknownTopicOrPartitionException("Partition %s doesn't exist on %d"
              .format(topicPartition, localBrokerId))
          }

          val numAppendedMessages =
            if (info.firstOffset == -1L || info.lastOffset == -1L)
              0
            else
              info.lastOffset - info.firstOffset + 1

          // update stats for successfully appended bytes and messages as bytesInRate and messageInRate
          BrokerTopicStats.getBrokerTopicStats(topicPartition.topic).bytesInRate.mark(messages.sizeInBytes)
          BrokerTopicStats.getBrokerAllTopicsStats.bytesInRate.mark(messages.sizeInBytes)
          BrokerTopicStats.getBrokerTopicStats(topicPartition.topic).messagesInRate.mark(numAppendedMessages)
          BrokerTopicStats.getBrokerAllTopicsStats.messagesInRate.mark(numAppendedMessages)

          trace("%d bytes written to log %s-%d beginning at offset %d and ending at offset %d"
            .format(messages.sizeInBytes, topicPartition.topic, topicPartition.partition, info.firstOffset, info.lastOffset))
          (topicPartition, LogAppendResult(info))
        } catch {
          // NOTE: Failed produce requests metric is not incremented for known exceptions
          // it is supposed to indicate un-expected failures of a broker in handling a produce request
          case e: KafkaStorageException =>
            fatal("Halting due to unrecoverable I/O error while handling produce request: ", e)
            Runtime.getRuntime.halt(1)
            (topicPartition, null)
          case e@ (_: UnknownTopicOrPartitionException |
                   _: NotLeaderForPartitionException |
                   _: RecordTooLargeException |
                   _: RecordBatchTooLargeException |
                   _: CorruptRecordException |
                   _: InvalidMessageException |
                   _: InvalidTimestampException) =>
            (topicPartition, LogAppendResult(LogAppendInfo.UnknownLogAppendInfo, Some(e)))
          case t: Throwable =>
            BrokerTopicStats.getBrokerTopicStats(topicPartition.topic).failedProduceRequestRate.mark()
            BrokerTopicStats.getBrokerAllTopicsStats.failedProduceRequestRate.mark()
            error("Error processing append operation on partition %s".format(topicPartition), t)
            (topicPartition, LogAppendResult(LogAppendInfo.UnknownLogAppendInfo, Some(t)))
        }
      }
    }
  }
```

## 基于Partition将batch数据写入磁盘

```java
def appendMessagesToLeader(messages: ByteBufferMessageSet, requiredAcks: Int = 0) = {
    //获取读锁
    val (info, leaderHWIncremented) = inReadLock(leaderIsrUpdateLock) {
      val leaderReplicaOpt = leaderReplicaIfLocal()
      //判断是否是leader partition
      leaderReplicaOpt match {
        case Some(leaderReplica) =>

          //获取分区对应的 Log
          val log = leaderReplica.log.get

          //配置定义的，表示 isr 中至少有多少个 replica。默认min.insync.replicas=1
          //如果配置为2，表示必须有一个 leader 和一个 follower
          val minIsr = log.config.minInSyncReplicas
          val inSyncSize = inSyncReplicas.size

          // Avoid writing to leader if there are not enough insync replicas to make it safe
          //如果当前 isr 列表数小于所配置最小isr数，并且 ack 是-1，则抛错
          //也就是没有足够的副本数来保证消息的可靠性
          if (inSyncSize < minIsr && requiredAcks == -1) {
            throw new NotEnoughReplicasException("Number of insync replicas for partition [%s,%d] is [%d], below required minimum [%d]"
              .format(topic, partitionId, inSyncSize, minIsr))
          }

          //基于 Log将消息写入磁盘文件
          val info = log.append(messages, assignOffsets = true)
          // probably unblock some follower fetch requests since log end offset has been updated
          //有消息写入 leader 之后，唤醒时间轮中的延时任务（fetch）
          replicaManager.tryCompleteDelayedFetch(new TopicPartitionOperationKey(this.topic, this.partitionId))
          // we may need to increment high watermark since ISR could be down to 1
          (info, maybeIncrementLeaderHW(leaderReplica))

        case None =>
          throw new NotLeaderForPartitionException("Leader not local for partition [%s,%d] on broker %d"
            .format(topic, partitionId, localBrokerId))
      }
    }

    // some delayed operations may be unblocked after HW changed
    if (leaderHWIncremented)
      tryCompleteDelayedRequests()

    info
  }
```

## 基于分区目录Log组件写入数据

调用分区对应的Log写入消息。将消息集合追加到log下的segment文件，如果有需要则重新新建一个segment文件。该方法也会负责给每条消息设置offset，如果assignOffsets=false标识存在，则只会检查已经存在的offset是否有效

```java
def append(messages: ByteBufferMessageSet, assignOffsets: Boolean = true): LogAppendInfo = {
    //校验消息格式并组装
    val appendInfo = analyzeAndValidateMessageSet(messages)

    // if we have any valid messages, append them to the log
    if (appendInfo.shallowCount == 0)
      return appendInfo

    // trim any invalid bytes or partial messages before appending it to the on-disk log
    var validMessages = trimInvalidBytes(messages, appendInfo)

    try {
      // they are valid, insert them in the log
      //对于一个分区目录而言，写入的时候都是进行并发控制的
      lock synchronized {

        if (assignOffsets) {
          //对于每个分区目录，在写入数据的时候，这个消息的 offset 都是顺序增长的
          // assign offsets to the message set
          //当前 LEO
          val offset = new LongRef(nextOffsetMetadata.messageOffset)
          appendInfo.firstOffset = offset.value
          val now = time.milliseconds
          val (validatedMessages, messageSizesMaybeChanged) = try {
            //校验并且设置offset
            validMessages.validateMessagesAndAssignOffsets(offset,
                                                           now,
                                                           appendInfo.sourceCodec,
                                                           appendInfo.targetCodec,
                                                           config.compact,
                                                           config.messageFormatVersion.messageFormatVersion,
                                                           config.messageTimestampType,
                                                           config.messageTimestampDifferenceMaxMs)
          } catch {
            case e: IOException => throw new KafkaException("Error in validating messages while appending to log '%s'".format(name), e)
          }
          validMessages = validatedMessages
          //消息最后的 offset
          appendInfo.lastOffset = offset.value - 1
          if (config.messageTimestampType == TimestampType.LOG_APPEND_TIME)
            appendInfo.timestamp = now

          // re-validate message sizes if there's a possibility that they have changed (due to re-compression or message
          // format conversion)
          if (messageSizesMaybeChanged) {
            for (messageAndOffset <- validMessages.shallowIterator) {
              if (MessageSet.entrySize(messageAndOffset.message) > config.maxMessageSize) {
                // we record the original message set size instead of the trimmed size
                // to be consistent with pre-compression bytesRejectedRate recording
                BrokerTopicStats.getBrokerTopicStats(topicAndPartition.topic).bytesRejectedRate.mark(messages.sizeInBytes)
                BrokerTopicStats.getBrokerAllTopicsStats.bytesRejectedRate.mark(messages.sizeInBytes)
                throw new RecordTooLargeException("Message size is %d bytes which exceeds the maximum configured message size of %d."
                  .format(MessageSet.entrySize(messageAndOffset.message), config.maxMessageSize))
              }
            }
          }

        } else {
          // we are taking the offsets we are given
          if (!appendInfo.offsetsMonotonic || appendInfo.firstOffset < nextOffsetMetadata.messageOffset)
            throw new IllegalArgumentException("Out of order offsets found in " + messages)
        }

        //最大不能超过1G
        // check messages set size may be exceed config.segmentSize
        if (validMessages.sizeInBytes > config.segmentSize) {
          throw new RecordBatchTooLargeException("Message set size is %d bytes which exceeds the maximum configured segment size of %d."
            .format(validMessages.sizeInBytes, config.segmentSize))
        }

        //如果一个 segment 写满了，此时创建新的 segment file
        // maybe roll the log if this segment is full
        val segment = maybeRoll(validMessages.sizeInBytes)

        //基于 segment 来写入消息到磁盘文件中
        // now append to the log
        segment.append(appendInfo.firstOffset, validMessages)

        //更新 LEO
        // increment the log end offset
        updateLogEndOffset(appendInfo.lastOffset + 1)

        trace("Appended message set to log %s with first offset: %d, next offset: %d, and messages: %s"
          .format(this.name, appendInfo.firstOffset, nextOffsetMetadata.messageOffset, validMessages))

        //os cache flush 磁盘
        //距离上次 flush 后，通过与 LEO 计算，未 flush 消息的数量超过所配置（flush.messages默认10000）后强制 flush
        if (unflushedMessages >= config.flushInterval)
          flush()

        appendInfo
      }
    } catch {
      case e: IOException => throw new KafkaStorageException("I/O exception in append to log '%s'".format(name), e)
    }
  }
```

## 基于segment文件写入数据

三种情况会创建新的segment文件：
1. 当前segment文件容量满，默认是1G
2. 超过一定时间，需要新建segment文件，默认1小时
3. 对应的index文件满

```java
private def maybeRoll(messagesSize: Int): LogSegment = {
    val segment = activeSegment
    //segmentSize配置默认是1G
    //判断是否需要新创建 segment
    if (segment.size > config.segmentSize - messagesSize ||
        segment.size > 0 && time.milliseconds - segment.created > config.segmentMs - segment.rollJitterMs ||
        segment.index.isFull) {
      debug("Rolling new log segment in %s (log_size = %d/%d, index_size = %d/%d, age_ms = %d/%d)."
            .format(name,
                    segment.size,
                    config.segmentSize,
                    segment.index.entries,
                    segment.index.maxEntries,
                    time.milliseconds - segment.created,
                    config.segmentMs - segment.rollJitterMs))
      roll()
    } else {
      segment
    }
  }
```

```java
def roll(): LogSegment = {
    val start = time.nanoseconds
    lock synchronized {
      //当前 partition 的 LEO 就是消息的下一个 offset，可以直接作为新的segment file 起始 offset 使用，也就是文件名
      val newOffset = logEndOffset
      //.log 与.index 文件名规则一致
      val logFile = logFilename(dir, newOffset)
      val indexFile = indexFilename(dir, newOffset)
      //如果文件已经存在则删除
      for(file <- List(logFile, indexFile); if file.exists) {
        warn("Newly rolled segment file " + file.getName + " already exists; deleting it first")
        file.delete()
      }

      segments.lastEntry() match {
        case null =>
        case entry => {
          entry.getValue.index.trimToValidSize()
          entry.getValue.log.trim()
        }
      }

      //LogSegment底层构造函数中，会将 dir 地址文件映射成FileChannel保存，供后续 .log 日志的写入
      val segment = new LogSegment(dir,
                                   startOffset = newOffset,
                                   indexIntervalBytes = config.indexInterval,
                                   maxIndexSize = config.maxIndexSize,
                                   rollJitterMs = config.randomSegmentJitter,
                                   time = time,
                                   fileAlreadyExists = false,
                                   initFileSize = initFileSize,
                                   preallocate = config.preallocate)
      //构造 segment 并添加到集合维护
      val prev = addSegment(segment)
      if(prev != null)
        throw new KafkaException("Trying to roll a new log segment for topic partition %s with start offset %d while it already exists.".format(name, newOffset))
      // We need to update the segment base offset and append position data of the metadata when log rolls.
      // The next offset should not change.
      //更新LEO
      updateLogEndOffset(nextOffsetMetadata.messageOffset)
      // schedule an asynchronous flush of the old segment
      scheduler.schedule("flush-log", () => flush(newOffset), delay = 0L)

      info("Rolled new log segment for '" + name + "' in %.0f ms.".format((System.nanoTime - start) / (1000.0*1000.0)))

      segment
    }
  }
```

Log会持有分区下所有的segment文件

```java
def addSegment(segment: LogSegment) = this.segments.put(segment.baseOffset, segment)
```

新的文件会更新LEO元数据

```java
private def updateLogEndOffset(messageOffset: Long) {
nextOffsetMetadata = new LogOffsetMetadata(messageOffset, activeSegment.baseOffset, activeSegment.size.toInt)
}
```

messageOffset是消息offset，segmentBaseOffset是当前segment起始offset，relativePositionInSegment是当前segment文件已经写入物理字节数
```java
case class LogOffsetMetadata(messageOffset: Long,
                             segmentBaseOffset: Long = LogOffsetMetadata.UnknownSegBaseOffset,
                             relativePositionInSegment: Int = LogOffsetMetadata.UnknownFilePosition) {
	...
}
```

## 基于segment机制将batch数据写入磁盘

Kafka 中的索引文件，以稀疏索引(sparse index)的方式构造消息的索引，它并不保证每个消息在索引文件中都有对应的索引项。每当写入一定量（由 broker 端参数log.index.interval.bytes 指定，默认值为 4096，即 4KB）的消息时，偏移量索引文件和时间戳索引文件分别增加一个偏移量索引项和时间戳索引项，增大或减小log.index.interval.bytes 的值，对应地可以缩小或增加索引项的密度。稀疏索引通过 MappedByteBuffer 将索引文件映射到内存中，以加快索引的查询速度

```java
@nonthreadsafe
  def append(offset: Long, messages: ByteBufferMessageSet) {

    if (messages.sizeInBytes > 0) {
      trace("Inserting %d bytes at offset %d at position %d".format(messages.sizeInBytes, offset, log.sizeInBytes()))
      // append an entry to the index (if needed)
      //每当写入的消息字节数大于所配置的4096字节数，则写入一条稀疏索引
      if(bytesSinceLastIndexEntry > indexIntervalBytes) {
        //写入逻辑 offset-->数据物理存储位置（log 一直在写入数据，因此总的字节数会对应递增，也就是磁盘所在的物理位置）
        index.append(offset, log.sizeInBytes())
        this.bytesSinceLastIndexEntry = 0
      }
      // append the messages
      //消息追加到.log 文件中
      //消息格式见Message类说明文档
      log.append(messages)
      this.bytesSinceLastIndexEntry += messages.sizeInBytes
    }
  }
```

基于FileChannel将消息数据写入到.log磁盘文件

```java
def append(messages: ByteBufferMessageSet) {
    //ByteBufferMessageSet封装了这个分区本次要写入的所有数据，调用writeFullyTo方法就是把这些数据写入到一个 Channel 里去
    //Channel底层一定是映射到磁盘文件，而且也会基于 os cache 写入
    val written = messages.writeFullyTo(channel)
    _size.getAndAdd(written)
  }
```

```java
/** Write the messages in this set to the given channel */
  def writeFullyTo(channel: GatheringByteChannel): Int = {
    //需要写入的数据都是存在于 ByteBuffer 中，这里都是基于 Java NIO 的操作，涉及到网络通信、磁盘读写、内存管理
    //mark 操作对当前 position做一个标记，后续读写对应的 position 都会变化，可以执行 reset 操作将 position 恢复到 mark 标记的位置
    buffer.mark()
    var written = 0
    while (written < sizeInBytes)
      //基于 FileChannel 写入数据也会先写入到 os cache，不会直接进入到磁盘文件，后续执行 flush 操作后才会真正写入磁盘
      written += channel.write(buffer)
    buffer.reset()
    written
  }
```

## 稀疏索引写入.index机制

基于Mmap将索引数据写入到.index磁盘文件

每8个字节就是一条稀疏索引，写入2个 Int类型数据，一个是基于当前 segment 相对逻辑 offset（LEO offset减去当前segment的base offset），一个是物理 offset

```java
def append(offset: Long, position: Int) {
    inLock(lock) {
      require(!isFull, "Attempt to append to a full index (size = " + _entries + ").")
      if (_entries == 0 || offset > _lastOffset) {
        debug("Adding index entry %d => %d to %s.".format(offset, position, _file.getName))
        //把.index 文件映射到 os 内存中，针对它的读写都是基于 os cache 来完成的
        //首先写入的是基于当前 segment file 的相对逻辑 offset
        mmap.putInt((offset - baseOffset).toInt)
        mmap.putInt(position)
        _entries += 1
        _lastOffset = offset
        require(_entries * 8 == mmap.position, _entries + " entries but file position in index is " + mmap.position + ".")
      } else {
        throw new InvalidOffsetException("Attempt to append an offset (%d) to position %d no larger than the last offset appended (%d) to %s."
          .format(offset, _entries, _lastOffset, _file.getAbsolutePath))
      }
    }
  }
```

## 更新LEO offset

在消息写入磁盘后，需要更新LEO offset，使得后续消息能够获取到正确的offset

```java
updateLogEndOffset(appendInfo.lastOffset + 1)
```

```java
private def updateLogEndOffset(messageOffset: Long) {
nextOffsetMetadata = new LogOffsetMetadata(messageOffset, activeSegment.baseOffset, activeSegment.size.toInt)
}
```

## 磁盘文件强制flush

.log和.index文件数据，都会基于FileChannel写入OS cache内存中，一定时间后强制flush到磁盘上，来提升系统性能

执行flush条件为：
1. 写入消息数量达到log.flush.interval.messages（默认10000）
2. 消息超过log.flush.interval.ms（默认为空）时间未执行flush
3. 定时任务log.flush.scheduler.interval.ms（默认不开启），将超过flush.ms（默认为空）未flush的数据执行flush操作

```java
//os cache flush 磁盘
//距离上次 flush 后，通过与 LEO 计算，未 flush 消息的数量超过所配置（flush.messages默认10000）后强制 flush
if (unflushedMessages >= config.flushInterval)
  flush()
```

遍历需要执行flush的segment

```java
def flush(offset: Long) : Unit = {
    if (offset <= this.recoveryPoint)
      return
    debug("Flushing log '" + name + " up to offset " + offset + ", last flushed: " + lastFlushTime + " current time: " +
          time.milliseconds + " unflushed = " + unflushedMessages)
    for(segment <- logSegments(this.recoveryPoint, offset))
      //遍历所有segment，对需要 flush 的segment进行处理
      segment.flush()
    lock synchronized {
      if(offset > this.recoveryPoint) {
        this.recoveryPoint = offset
        lastflushedTime.set(time.milliseconds)
      }
    }
  }
```

```java
@threadsafe
  def flush() {
    LogFlushStats.logFlushTimer.time {
      log.flush()
      index.flush()
    }
  }
```

```java
//FileChannel的flush方法
def flush() = {
	channel.force(true)
}	

//Mmap的flush方法
def flush() {
	inLock(lock) {
  		mmap.force()
	}
}
```

## 消息写入磁盘后更新HW

在消息写入成功后，可能需要更新HW。需要注意的是，如果ISR 副本数配置的是1，并且 ack 为默认的1，则当 leader 写入消息后，更新完 LEO，就会更新 HW

```java
private def maybeIncrementLeaderHW(leaderReplica: Replica): Boolean = {
    val allLogEndOffsets = inSyncReplicas.map(_.logEndOffset)
    //计算所有 ISR 中最小的 LEO
    val newHighWatermark = allLogEndOffsets.min(new LogOffsetMetadata.OffsetOrdering)
    //目前 leader replica 的 HW
    val oldHighWatermark = leaderReplica.highWatermark
    //判断是否需要更新 leader replica 的 HW
    //1.如果ISR列表中所有副本的LEO都大于当前leader的HW，则更新
    //2.如果leader的HW已经在旧的segment则更新
    if (oldHighWatermark.messageOffset < newHighWatermark.messageOffset || oldHighWatermark.onOlderSegment(newHighWatermark)) {
      leaderReplica.highWatermark = newHighWatermark
      debug("High watermark for partition [%s,%d] updated to %s".format(topic, partitionId, newHighWatermark))
      true
    } else {
      debug("Skipping update high watermark since Old hw %s is larger than new hw %s for partition [%s,%d]. All leo's are %s"
        .format(oldHighWatermark, newHighWatermark, topic, partitionId, allLogEndOffsets.mkString(",")))
      false
    }
  }
```

# 消息写入成功后的回调

```java
//判断是否需要等待其他replicate返回响应
if (delayedRequestRequired(requiredAcks, messagesPerPartition, localProduceResults)) {
// create delayed produce operation
val produceMetadata = ProduceMetadata(requiredAcks, produceStatus)
val delayedProduce = new DelayedProduce(timeout, produceMetadata, this, responseCallback)

// create a list of (topic, partition) pairs to use as keys for this delayed produce operation
val producerRequestKeys = messagesPerPartition.keys.map(new TopicPartitionOperationKey(_)).toSeq

// try to complete the request immediately, otherwise put it into the purgatory
// this is because while the delayed produce operation is being created, new
// requests may arrive and hence make this operation completable.
delayedProducePurgatory.tryCompleteElseWatch(delayedProduce, producerRequestKeys)

} else {
// we can respond immediately
// 不需要等待其他replicate结果，直接回调并返回
val produceResponseStatus = produceStatus.mapValues(status => status.responseStatus)
responseCallback(produceResponseStatus)
}
```

判断是否需要发送一个延迟请求，用于等待其他副本处理完成（依据acks)

```java
private def delayedRequestRequired(requiredAcks: Short, messagesPerPartition: Map[TopicPartition, MessageSet],
                                       localProduceResults: Map[TopicPartition, LogAppendResult]): Boolean = {
    requiredAcks == -1 &&
    messagesPerPartition.size > 0 &&
    localProduceResults.values.count(_.error.isDefined) < messagesPerPartition.size
  }
```

消息写入完成后，会执行对应的回调，来处理写入的结果。KafkaApis封装好了处理结果的逻辑

```java
// the callback for sending a produce response
//每个请求都是发送给同一个broker，一个broker上可能有多个leader partition
//所以请求中可能是多个partition-->batch的对应
//所以在执行请求的时候，将每个分区的Batch数据追加到对应分区的磁盘文件中，最终每个分区都对应一个结果
def sendResponseCallback(responseStatus: Map[TopicPartition, PartitionResponse]) {

  val mergedResponseStatus = responseStatus ++ unauthorizedRequestInfo.mapValues(_ =>
    new PartitionResponse(Errors.TOPIC_AUTHORIZATION_FAILED.code, -1, Message.NoTimestamp))

  var errorInResponse = false

  mergedResponseStatus.foreach { case (topicPartition, status) =>
    if (status.errorCode != Errors.NONE.code) {
      errorInResponse = true
      debug("Produce request with correlation id %d from client %s on partition %s failed due to %s".format(
        request.header.correlationId,
        request.header.clientId,
        topicPartition,
        Errors.forCode(status.errorCode).exceptionName))
    }
  }

  //响应最终会执行该回调
  //不管acks配置的值是多少，都会封装为Response放入到对应的队列中
  def produceResponseCallback(delayTimeMs: Int) {
    //依据 acks 回调不同的逻辑
    if (produceRequest.acks == 0) {
      // no operation needed if producer request.required.acks = 0; however, if there is any error in handling
      // the request, since no response is expected by the producer, the server will close socket server so that
      // the producer client will know that some error has happened and will refresh its metadata
      if (errorInResponse) {
        val exceptionsSummary = mergedResponseStatus.map { case (topicPartition, status) =>
          topicPartition -> Errors.forCode(status.errorCode).exceptionName
        }.mkString(", ")
        info(
          s"Closing connection due to error during produce request with correlation id ${request.header.correlationId} " +
            s"from client id ${request.header.clientId} with ack=0\n" +
            s"Topic and partition to exceptions: $exceptionsSummary"
        )
        requestChannel.closeConnection(request.processor, request)
      } else {
        //将回调结果返回至responseQueues队列
        requestChannel.noOperation(request.processor, request)
      }
    } else {
      val respHeader = new ResponseHeader(request.header.correlationId)
      val respBody = request.header.apiVersion match {
        case 0 => new ProduceResponse(mergedResponseStatus.asJava)
        case version@(1 | 2) => new ProduceResponse(mergedResponseStatus.asJava, delayTimeMs, version)
        // This case shouldn't happen unless a new version of ProducerRequest is added without
        // updating this part of the code to handle it properly.
        case version => throw new IllegalArgumentException(s"Version `$version` of ProduceRequest is not handled. Code must be updated.")
      }
      //将回调结果返回至responseQueues队列
      requestChannel.sendResponse(new RequestChannel.Response(request, new ResponseSend(request.connectionId, respHeader, respBody)))
    }
  }
```

# LogManager组件管理Log磁盘文件

KafkaServer初始化的时候，也会初始化LogManager组件，用来启动后台线程定时flush数据到磁盘，和清理过期的磁盘数据

```java
/* start log manager */
logManager = createLogManager(zkUtils.zkClient, brokerState)
logManager.startup()
```

```java
def startup() {
    /* Schedule the cleanup task to delete old logs */
    if(scheduler != null) {
      info("Starting log cleanup with a period of %d ms.".format(retentionCheckMs))
      //每隔log.retention.check.interval.ms（默认5分钟）执行清理任务
      scheduler.schedule("kafka-log-retention", 
                         cleanupLogs, 
                         delay = InitialTaskDelayMs, 
                         period = retentionCheckMs, 
                         TimeUnit.MILLISECONDS)
      info("Starting log flusher with a default period of %d ms.".format(flushCheckMs))
      //每隔log.flush.scheduler.interval.ms（默认无限大，不执行）定期执行flush操作
      scheduler.schedule("kafka-log-flusher", 
                         flushDirtyLogs, 
                         delay = InitialTaskDelayMs, 
                         period = flushCheckMs, 
                         TimeUnit.MILLISECONDS)
      //每隔log.flush.offset.checkpoint.interval.ms（默认60s）,更新分区文件夹下的recovery-point-offset-checkpoint文件，保存已经写入磁盘的offset
      //也就是从recoveryPoint开始，还没有flush到磁盘数据
      scheduler.schedule("kafka-recovery-point-checkpoint",
                         checkpointRecoveryPointOffsets,
                         delay = InitialTaskDelayMs,
                         period = flushCheckpointMs,
                         TimeUnit.MILLISECONDS)
    }
    if(cleanerConfig.enableCleaner)
      cleaner.startup()
  }
```

清理过期的磁盘数据

```java
def cleanupLogs() {
    debug("Beginning log cleanup...")
    var total = 0
    val startMs = time.milliseconds
    for(log <- allLogs; if !log.config.compact) {
      debug("Garbage collecting '" + log.name + "'")
      //删除超过retention.ms（默认7天）的segment文件
      //删除容量超过retention.bytes（默认-1不限制）的segment文件
      total += cleanupExpiredSegments(log) + cleanupSegmentsToMaintainSize(log)
    }
    debug("Log cleanup completed. " + total + " files deleted in " +
                  (time.milliseconds - startMs) / 1000 + " seconds")
  }	
```

定期将segment文件flush到磁盘

```java
private def flushDirtyLogs() = {
    debug("Checking for dirty logs to flush...")

    for ((topicAndPartition, log) <- logs) {
      try {
        val timeSinceLastFlush = time.milliseconds - log.lastFlushTime
        debug("Checking if flush is needed on " + topicAndPartition.topic + " flush interval  " + log.config.flushMs +
              " last flushed " + log.lastFlushTime + " time since last flush: " + timeSinceLastFlush)
        if(timeSinceLastFlush >= log.config.flushMs)
          log.flush
      } catch {
        case e: Throwable =>
          error("Error flushing topic " + topicAndPartition.topic, e)
      }
    }
  }
```