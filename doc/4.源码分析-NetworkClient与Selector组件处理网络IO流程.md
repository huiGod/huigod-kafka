[TOC]

# NetworkClient初始化流程

NetworkClient是进行异步网络IO操作的客户端，在生产者KafkaProducer客户端初始化时，会对NetworkClient进行初始化操作

KafkaProducer客户端初始化时构造NetworkClient源码：

```java
//网络通信组件
NetworkClient client = new NetworkClient(
        //对一个 broker 的连接最大空闲时间connections.max.idle.ms默认9分钟，超过需要回收
        new Selector(config.getLong(ProducerConfig.CONNECTIONS_MAX_IDLE_MS_CONFIG), this.metrics, time, "producer", channelBuilder),
        this.metadata,
        clientId,
        //配置max.in.flight.requests.per.connection，默认是5，表示对同一个 broker 最多允许多少个 request 发送后无响应
        //如果配置大于1，消息重试后可能会造成乱序
        config.getInt(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION),
        //对 broker 连接失败的重试间隔reconnect.backoff.ms，默认50ms
        config.getLong(ProducerConfig.RECONNECT_BACKOFF_MS_CONFIG),
        //socket 默认发送请求缓冲区send.buffer.bytes，128k
        config.getInt(ProducerConfig.SEND_BUFFER_CONFIG),
        //socket 默认接收请求缓冲区send.buffer.bytes，128k
        config.getInt(ProducerConfig.RECEIVE_BUFFER_CONFIG),
        this.requestTimeoutMs, time);
```

NetworkClient构造流程

```java
public class NetworkClient implements KafkaClient {

    private static final Logger log = LoggerFactory.getLogger(NetworkClient.class);

    //用于监听IO事件
    private final Selectable selector;

    private final MetadataUpdater metadataUpdater;

    private final Random randOffset;

    //集群中每个节点的连接状态
    private final ClusterConnectionStates connectionStates;

    //已经成功发送等待响应的请求集合
    private final InFlightRequests inFlightRequests;

    /* the socket send buffer size in bytes */
    private final int socketSendBuffer;

    /* the socket receive size buffer in bytes */
    private final int socketReceiveBuffer;

    /* the client id used to identify this client in requests to the server */
    private final String clientId;

    /* the current correlation id to use when sending requests to servers */
    private int correlation;

    /* max time in ms for the producer to wait for acknowledgement from server*/
    private final int requestTimeoutMs;

    private final Time time;

    ...

    private NetworkClient(MetadataUpdater metadataUpdater,
                          Metadata metadata,
                          Selectable selector,
                          String clientId,
                          int maxInFlightRequestsPerConnection,
                          long reconnectBackoffMs,
                          int socketSendBuffer,
                          int socketReceiveBuffer,
                          int requestTimeoutMs,
                          Time time) {

        /* It would be better if we could pass `DefaultMetadataUpdater` from the public constructor, but it's not
         * possible because `DefaultMetadataUpdater` is an inner class and it can only be instantiated after the
         * super constructor is invoked.
         */
        if (metadataUpdater == null) {
            if (metadata == null)
                throw new IllegalArgumentException("`metadata` must not be null");
            this.metadataUpdater = new DefaultMetadataUpdater(metadata);
        } else {
            this.metadataUpdater = metadataUpdater;
        }
        this.selector = selector;
        this.clientId = clientId;
        this.inFlightRequests = new InFlightRequests(maxInFlightRequestsPerConnection);
        this.connectionStates = new ClusterConnectionStates(reconnectBackoffMs);
        this.socketSendBuffer = socketSendBuffer;
        this.socketReceiveBuffer = socketReceiveBuffer;
        this.correlation = 0;
        this.randOffset = new Random();
        this.requestTimeoutMs = requestTimeoutMs;
        this.time = time;
    }

```

# Selector网络组件

Selector是自定义的组件，用于使用NIO非阻塞处理多路复用IO操作，该类用于处理网络请求NetworkSend和响应NetworkReceive

Selector初始化源码：

```java
public class Selector implements Selectable {

    private static final Logger log = LoggerFactory.getLogger(Selector.class);

    //封装了原生的Java NIO的Selector，多路复用组件，一个线程调用他直接监听多个网络连接的请求和响应
    private final java.nio.channels.Selector nioSelector;
    //broker id到Channel的映射关系。每个broker都有一个网络连接，每个连接在NIO的语义里，都有一个对应的SocketChannel，KafkaChannel封装了SocketChannel
    private final Map<String, KafkaChannel> channels;
    //已经成功发送出去的请求
    private final List<Send> completedSends;
    //已经接收回来的响应而且被处理完了
    private final List<NetworkReceive> completedReceives;
    //每个Broker的收到的但是还没有被处理的响应
    private final Map<KafkaChannel, Deque<NetworkReceive>> stagedReceives;
    private final Set<SelectionKey> immediatelyConnectedKeys;
    private final List<String> disconnected;
    private final List<String> connected;
    private final List<String> failedSends;
    private final Time time;
    private final SelectorMetrics sensors;
    private final String metricGrpPrefix;
    private final Map<String, String> metricTags;
    private final ChannelBuilder channelBuilder;
    private final Map<String, Long> lruConnections;
    //每个网络连接最多可以空闲的时间的大小，就要回收掉
    private final long connectionsMaxIdleNanos;
    //最大可以接收的数据量的大小
    private final int maxReceiveSize;
    private final boolean metricsPerConnection;
    private long currentTimeNanos;
    private long nextIdleCloseCheckTime;


    /**
     * Create a new nioSelector
     */
    public Selector(int maxReceiveSize, long connectionMaxIdleMs, Metrics metrics, Time time, String metricGrpPrefix, Map<String, String> metricTags, boolean metricsPerConnection, ChannelBuilder channelBuilder) {
        try {
            this.nioSelector = java.nio.channels.Selector.open();
        } catch (IOException e) {
            throw new KafkaException(e);
        }
        this.maxReceiveSize = maxReceiveSize;
        this.connectionsMaxIdleNanos = connectionMaxIdleMs * 1000 * 1000;
        this.time = time;
        this.metricGrpPrefix = metricGrpPrefix;
        this.metricTags = metricTags;
        this.channels = new HashMap<>();
        this.completedSends = new ArrayList<>();
        this.completedReceives = new ArrayList<>();
        this.stagedReceives = new HashMap<>();
        this.immediatelyConnectedKeys = new HashSet<>();
        this.connected = new ArrayList<>();
        this.disconnected = new ArrayList<>();
        this.failedSends = new ArrayList<>();
        this.sensors = new SelectorMetrics(metrics);
        this.channelBuilder = channelBuilder;
        // initial capacity and load factor are default, we set them explicitly because we want to set accessOrder = true
        this.lruConnections = new LinkedHashMap<>(16, .75F, true);
        currentTimeNanos = time.nanoseconds();
        nextIdleCloseCheckTime = currentTimeNanos + connectionsMaxIdleNanos;
        this.metricsPerConnection = metricsPerConnection;
    }
    ...
}
```

# 客户端创建connect连接

客户端在发送数据给到broker时，会对网络连接进行如下初始化操作

TCP/IP协议中,无论发送多少数据,总是要在数据前面加上协议头,同时,对方接收到数据,也需要发送ACK表示确认。为了尽可能的利用网络带宽,TCP总是希望尽可能的发送足够大的数据.(在一个连接中会设置MSS参数,因此,TCP/IP希望每次都能够以MSS尺寸的数据块来发送数据).Nagle算法就是为了尽可能发送大块数据,避免网络中充斥着许多小数据块。Nagle算法的基本定义是任意时刻,最多只能有一个未被确认的小段. 所谓“小段”,指的是小于MSS尺寸的数据块,所谓“未被确认”,是指一个数据块发送出去后,没有收到对方发送的ACK确认该数据已收到

keepalive的意思，主要是避免客户端和服务端任何一方如果断开连接之后，别人不知道，一直保持着网络连接的资源；所以设置这个之后，2小时内如果双方没有任何通信，那么发送一个探测包，根据探测包的结果保持连接、重新连接或者断开连接

工业级的网络连接，也只是设置了KeepAlive、TcpNoDelay、SocketBuffer参数

```java
@Override
public void connect(String id, InetSocketAddress address, int sendBufferSize, int receiveBufferSize) throws IOException {
    //判断连接是否已经建立
    if (this.channels.containsKey(id))
        throw new IllegalStateException("There is already a connection for id " + id);

    SocketChannel socketChannel = SocketChannel.open();
    //配置为非阻塞
    socketChannel.configureBlocking(false);
    Socket socket = socketChannel.socket();
    //tcp 长连接开启心跳检测探测连接是否正常
    socket.setKeepAlive(true);
    if (sendBufferSize != Selectable.USE_DEFAULT_BUFFER_SIZE)
        //设置 tcp 发送数据缓冲区
        socket.setSendBufferSize(sendBufferSize);
    if (receiveBufferSize != Selectable.USE_DEFAULT_BUFFER_SIZE)
        //设置 tcp 接收数据缓冲区
        socket.setReceiveBufferSize(receiveBufferSize);
    //关闭Nagle's算法，让发送出去的数据包立即通过网络传输过去
    socket.setTcpNoDelay(true);
    boolean connected;
    try {
        //创建网络连接
        //一个channel在非阻塞模式下执行connect后，如果连接能马上建立好则返回true(连接本地可能会很快成功)，否则完成false
        //如果返回false，那么只能通过之后调用finishConnect来判断连接是否完成。
        connected = socketChannel.connect(address);
    } catch (UnresolvedAddressException e) {
        socketChannel.close();
        throw new IOException("Can't resolve address: " + address, e);
    } catch (IOException e) {
        socketChannel.close();
        throw e;
    }
    //将创建的连接注册到 Selector 上，并且关注OP_CONNECT事件
    SelectionKey key = socketChannel.register(nioSelector, SelectionKey.OP_CONNECT);
    //可以通过SelectionKey.key获取到连接channel，最终封装为KafkaChannel
    KafkaChannel channel = channelBuilder.buildChannel(id, key, maxReceiveSize);
    //后续可以通过attachment()方法获取到 KafkaChannel
    //后续在通过SelectionKey进行网络请求和相应的处理的时候，就可以从SelectionKey里获取出来SocketChannel
    key.attach(channel);
    //将连接缓存
    this.channels.put(id, channel);

    if (connected) {
        // OP_CONNECT won't trigger for immediately connected channels
        log.debug("Immediately connected to node {}", channel.id());
        immediatelyConnectedKeys.add(key);
        key.interestOps(0);
    }
}
```

# Sender线程调用NetworkClient处理poll流程

Sender线程的run方法每次执行完数据组装后，调用NetworkClient的poll方法来处理网络IO相关操作

```java
@Override
    public List<ClientResponse> poll(long timeout, long now) {
        //metadataUpdater是更新元数据组件，如果满足条件会发送更新元数据请求
        long metadataTimeout = metadataUpdater.maybeUpdate(now);
        try {
            //多路复用处理网络IO操作
            this.selector.poll(Utils.min(timeout, metadataTimeout, requestTimeoutMs));
        } catch (IOException e) {
            log.error("Unexpected error during I/O", e);
        }

        // process completed actions
        long updatedNow = this.time.milliseconds();
        List<ClientResponse> responses = new ArrayList<>();

        //completedSends队列和completedReceives队列只会保存同一个channel对应的一个完整消息

        //处理已经发送成功的后续操作
        handleCompletedSends(responses, updatedNow);
        //接收到 broker 响应后执行回调操作
        handleCompletedReceives(responses, updatedNow);
        //处理连接断开
        handleDisconnections(responses, updatedNow);
        //处理连接成功的节点
        handleConnections();
        //处理超时请求
        handleTimedOutRequests(responses, updatedNow);

        // invoke callbacks
        //执行消息回调
        for (ClientResponse response : responses) {
            if (response.request().hasCallback()) {
                try {
                    response.request().callback().onComplete(response);
                } catch (Exception e) {
                    log.error("Uncaught error in request completion:", e);
                }
            }
        }

        return responses;
    }
```

## Selector组件多路复用处理网络IO

NetworkClient调用的是Selector的poll操作来进行多路复用的网络IO流程处理

```java
@Override
public void poll(long timeout) throws IOException {
    if (timeout < 0)
        throw new IllegalArgumentException("timeout should be >= 0");

    //清空队列，对IO操作中间的队列数据结构清理
    clear();

    if (hasStagedReceives() || !immediatelyConnectedKeys.isEmpty())
        timeout = 0;

    /* check ready keys */
    long startSelect = time.nanoseconds();
    //调用底层nio 组件Selector.select，如果所有 socketChannel 都没有 IO 读写事件发生，最多阻塞指定时间
    int readyKeys = select(timeout);
    long endSelect = time.nanoseconds();
    currentTimeNanos = endSelect;
    this.sensors.selectTime.record(endSelect - startSelect, time.milliseconds());

    //如果有任一个socketChannel有 IO 读写事件发生，则readyKeys>0
    if (readyKeys > 0 || !immediatelyConnectedKeys.isEmpty()) {
        //处理IO 读写事件具体逻辑
        pollSelectionKeys(this.nioSelector.selectedKeys(), false);
        pollSelectionKeys(immediatelyConnectedKeys, true);
    }

    //将每个KafkaChannel接受到的第一个完整响应数据从stagedReceives队列中放入到completedReceives队列中
    addToCompletedReceives();

    long endIo = time.nanoseconds();
    this.sensors.ioTime.record(endIo - endSelect, time.milliseconds());
    //通过lru关闭空间时间较长的连接
    maybeCloseOldestConnection();
}
```

### SelectionKey遍历操作

```java
private void pollSelectionKeys(Iterable<SelectionKey> selectionKeys, boolean isImmediatelyConnected) {
    Iterator<SelectionKey> iterator = selectionKeys.iterator();
    //遍历所有有事件发生的SelectionKey
    while (iterator.hasNext()) {
        SelectionKey key = iterator.next();
        //从集合中移除当前SelectionKey
        iterator.remove();
        //在创建连接时，注册到Selector 后执行了attach操作。这里对应可以通过attachment来获取
        KafkaChannel channel = channel(key);

        // register all per-connection metrics at once
        sensors.maybeRegisterConnectionMetrics(channel.id());
        //保存连接-->发生事件时的时间
        //lruConnections，因为一般来说一个客户端不能放太多的Socket连接资源，否则会导致这个客户端的负载过重，
        //所以他需要采用lru的方式来不断的淘汰掉最近最少使用的一些连接，很多连接最近没怎么发送消息
        lruConnections.put(channel.id(), currentTimeNanos);

        try {

            /* complete any connections that have finished their handshake (either normally or immediately) */
            //如果发现SelectionKey当前处于的状态是可以建立连接，isConnectable方法是true
            if (isImmediatelyConnected || key.isConnectable()) {
                //调用到KafkaChannel最底层的SocketChannel的finishConnect方法，等待这个连接必须执行完毕
                //连接创建成功，取消关注OP_CONNECT事件，并且关注OP_READ事件
                if (channel.finishConnect()) {
                    //维护所有已经创建好连接的id
                    this.connected.add(channel.id());
                    this.sensors.connectionCreated.record();
                } else
                    //因为是非阻塞IO，如果连接未成功创建好，则在下一次select会继续连接的创建
                    //也可以同步等待，上一步对finishConnect进行while等待直到连接创建好
                    continue;
            }

            /* if channel is not ready finish prepare */
            if (channel.isConnected() && !channel.ready())
                channel.prepare();

            /* if channel is ready read from any connections that have readable data */
            //处理网络读事件（接收请求）
            //如果已经读取了完整消息networkReceive，会放入到channel下的stagedReceives队列，后续需要stagedReceives队列处理完成才能继续读取下一个响应数据
            if (channel.ready() && key.isReadable() && !hasStagedReceive(channel)) {
                NetworkReceive networkReceive;
                //一个 broker 可以通过一个连接连续发送出去多个请求，这些请求可能都没有收到响应消息，此时 broker 端可能会连续处理完
                //多个请求然后连续返回多个响应，所以这里通过 while 循环连续不断的读响应数据
                while ((networkReceive = channel.read()) != null)
                    //将读取到的完整消息放入到channel对应的stagedReceives队列
                    addToStagedReceives(channel, networkReceive);
            }

            /* if channel is ready write to any sockets that have space in their buffer and for which we have data */
            //处理网络写事件（发送请求）
            //拆包现象：
            //如果Kafka一个请求一次write操作没有把全部的数据都写到broker去，相当于出现了类似于拆包的问题，一个请求一次没法发送完毕，此时如何处理的呢？
            //如果说一个请求对应的ByteBuffer中的二进制字节数据一次write没有全部发送完毕,底层ByteBufferSend的remaining是大于0，此时就不会取消对OP_WRITE事件的监听
            //如果出现拆包，这里的 send 返回的是 null
            //针对某个Broker，再次发送一个Request时，必须得先判断一下，这个Broker上一次发送的Request请求是否发送完毕了，并且还得限制为最多只发送5个request是没有收到响应的
            //如果说上一次 request出现了类似拆包的问题，一次请求没有发送完毕，此时下次就不会继续往这个broker发送请求了，但是此时针对这个broker还是保持着OP_WRITE的监听，下次调用poll，会发现对这个broker可以再次执行WRITABLE事件
            if (channel.ready() && key.isWritable()) {
                Send send = channel.write();
                if (send != null) {
                    //send数据全部发送成功才会放入到completedSends队列
                    this.completedSends.add(send);
                    this.sensors.recordBytesSent(channel.id(), send.size());
                }
            }

            /* cancel any defunct sockets */
            if (!key.isValid()) {
                close(channel);
                this.disconnected.add(channel.id());
            }

        } catch (Exception e) {
            String desc = channel.socketDescription();
            if (e instanceof IOException)
                log.debug("Connection with {} disconnected", desc, e);
            else
                log.warn("Unexpected error from {}; closing connection", desc, e);
            //读写任何异常（包括超时），都会关闭连接，释放各种资源。后续会更新拉取元数据标识
            close(channel);
            //需要关闭的连接添加到队列中
            this.disconnected.add(channel.id());
        }
    }
}
```

### read读操作

调用KafkaChannel的read方法来读取数据，需要注意的是拆包的处理，如果发生拆包，方法返回的是null，使得pollSelectionKeys方法中的while跳出执行，在下一次OP_READ事件发生时，则继续从网络连接中读取数据，读取到完整的响应数据NetworkReceive才返回，下一次重新开始读取下一个NetworkReceive

```java
if (channel.ready() && key.isReadable() && !hasStagedReceive(channel)) {
    NetworkReceive networkReceive;
    //一个 broker 可以通过一个连接连续发送出去多个请求，这些请求可能都没有收到响应消息，此时 broker 端可能会连续处理完
    //多个请求然后连续返回多个响应，所以这里通过 while 循环连续不断的读响应数据
    while ((networkReceive = channel.read()) != null)
        //将读取到的完整消息放入到channel对应的stagedReceives队列
        addToStagedReceives(channel, networkReceive);
}
```

```java
public NetworkReceive read() throws IOException {
    NetworkReceive result = null;

    if (receive == null) {
        //第一次读取数据初始化NetworkReceive对象
        receive = new NetworkReceive(maxReceiveSize, id);
    }

    //从网络连接中读取数据到NetworkReceive中
    receive(receive);
    //判断请求是否完整
    if (receive.complete()) {
        receive.payload().rewind();
        result = receive;
        //读取完成将NetworkReceive作为结果返回，并且重置receive，使得下一次可以重新读取数据到NetworkReceive
        receive = null;
    }
    //如果发生拆包，直接返回null，下一次OP_READ事件继续来处理
    return result;
}
```

receive底层调用的是readFromReadableChannel方法，这里的处理拆包与粘包逻辑是精华

粘包解决方式：size默认是4字节，读取一个 int 类型数据，代表下一次完整的请求体大小
拆包解决方式：NetworkReceive 对象继续存在，在下一次 poll 执行时，发现又有数据可以读取，则继续读取
拆包问题包含：size发生拆包、body请求体发生拆包，此方法解决了各种拆包与粘包问题

```java
@Deprecated
public long readFromReadableChannel(ReadableByteChannel channel) throws IOException {
    int read = 0;
    //size默认会分配ByteBuffer.allocate(4)
    if (size.hasRemaining()) {
        //读取4个字节数据写入到 size，此时 position=4
        int bytesRead = channel.read(size);
        if (bytesRead < 0)
            //idea断开连接会发送一个请求，read到的数据时-1，可以参考之类的处理方式
            throw new EOFException();
        read += bytesRead;
        if (!size.hasRemaining()) {
            //size是ByteBuffer,rewind可以理解为从之前的写设置为从头开始读读书
            size.rewind();
            //读取一个整型数字代表 body 请求体大小
            int receiveSize = size.getInt();
            if (receiveSize < 0)
                throw new InvalidReceiveException("Invalid receive (size = " + receiveSize + ")");
            if (maxSize != UNLIMITED && receiveSize > maxSize)
                throw new InvalidReceiveException("Invalid receive (size = " + receiveSize + " larger than " + maxSize + ")");

            //申请body数据大小的ByteBuffer，后续从channel读取数据该ByteBuffer
            this.buffer = ByteBuffer.allocate(receiveSize);
        }
    }
    if (buffer != null) {
        int bytesRead = channel.read(buffer);
        if (bytesRead < 0)
            throw new EOFException();
        read += bytesRead;
    }
    //返回的是已经读取的字节数，后续用来判断是否读取完成了
    return read;
}
```

判断请求是否读取完成，是否发生拆包是通过complete方法

```java
@Override
public boolean complete() {
    return !size.hasRemaining() && !buffer.hasRemaining();
}
```

### write写操作

数据发送给网络连接，调用的是KafkaChannel的write方法

```java
if (channel.ready() && key.isWritable()) {
    Send send = channel.write();
    if (send != null) {
        //send数据全部发送成功才会放入到completedSends队列
        this.completedSends.add(send);
        this.sensors.recordBytesSent(channel.id(), send.size());
    }
}
```

如果说已经发送完毕数据了，那么就可以取消对OP_WRITE事件的关注，否则如果一个Request的数据都没发送完毕，此时还需要保持对OP_WRITE事件的关注,而且如果发送完毕了，就会放到completedSends里面去

```java
public Send write() throws IOException {
    Send result = null;
    //需要发送的请求RequestSend会绑定到KafkaChannel，所以每次只会发送一个send
    if (send != null && send(send)) {
        //返回成功发送的 send 数据
        result = send;
        //发送成功后清空当前 KafkaChannel的 send
        send = null;
    }
    //如果成功发送，则返回已经发送的Send
    //如果发送拆包，未发送完成，则返回null
    return result;
}
```

```java
private boolean send(Send send) throws IOException {
    //如果发生拆包，下一次会接着继续写
    send.writeTo(transportLayer);
    //如果数据发送完成，移除对OP_WRITE事件的关注
    if (send.completed())
        //客户端在组装需要发送的数据时，绑定Send后会关注OP_WRITE事件
        transportLayer.removeInterestOps(SelectionKey.OP_WRITE);

    //返回是否完成发送，如果发生拆包则返回false
    return send.completed();
}
```

### 将读取到的完整请求放入到completedReceives队列

如果一个连接一次OP_READ读取出来多个响应消息的话，在这里仅仅只会把每个连接对应的第一个响应消息会放到completedReceives里面去，供后续处理，此时有可能某个连接的stagedReceives是不为空的。stagedReceives不为空，则下一次poll不会执行read操作，但是仍然会将队头第一个响应消息放入到completedReceives中，供后续处理响应并从inFlightRequests中移除

```java
private void addToCompletedReceives() {
    if (!this.stagedReceives.isEmpty()) {
        Iterator<Map.Entry<KafkaChannel, Deque<NetworkReceive>>> iter = this.stagedReceives.entrySet().iterator();
        while (iter.hasNext()) {
            Map.Entry<KafkaChannel, Deque<NetworkReceive>> entry = iter.next();
            KafkaChannel channel = entry.getKey();
            //只有关注 OP_READ 事件，才会进行处理
            //RequestChannel的requestQueue队列在保存一个请求数据后，移除OP_READ 事件，这里就不会将数据继续放入到completedReceives中
            //只有待requestQueue队列处理完一个请求后，才能继续从stagedReceives获取数据放入到completedReceives中
            if (!channel.isMute()) {
                Deque<NetworkReceive> deque = entry.getValue();
                NetworkReceive networkReceive = deque.poll();
                this.completedReceives.add(networkReceive);
                this.sensors.recordBytesReceived(channel.id(), networkReceive.payload().limit());
                //当队列为空会删除队列，只有删除了在poll中该channel才会继续读取数据
                if (deque.isEmpty())
                    iter.remove();
            }
        }
    }
}
```

## handleCompletedSends处理已发送完的消息

一个 kafkaChannel 只能绑定一个 send，因此一次 IO 处理流程，对同一个 broker 也一定只能发送一个 send这里获取的inFlightRequests队列队头元素也一定是刚刚发送完的send。因为发送完成才能够去发送下一个send，才会放入到inFlightRequests队列中去

```java
private void handleCompletedSends(List<ClientResponse> responses, long now) {
    // if no response is expected then when the send is completed, return it
    for (Send send : this.selector.completedSends()) {
        //从InFlightRequests队列获取channel对应队列的队头元素（只是peek没有poll）
        //最近发送出去的消息放在InFlightRequests的队头，这里获取到的请求是最近发送出去的请求
        ClientRequest request = this.inFlightRequests.lastSent(send.destination());
        //通过acks计算是否需要等待请求的响应，如果不需要这里可以直接从inFlightRequests队列里面移出去
        //acks != 0 == expectResponse
        if (!request.expectResponse()) {
            //如果不需要返回响应直接poll掉即可
            this.inFlightRequests.completeLastSent(send.destination());
            //直接封装响应
            responses.add(new ClientResponse(request, now, false, null));
        }
    }
}
```
## handleCompletedReceives处理已接收到的消息

对于同一个broker，连续发送多个request出去，但是会在inFlighRequest里面排队，并且是从队头开始放入例如FlighRequests -> <请求5，请求4，请求3，请求2，请求1>，此时对broker读取响应，加入一次性读取到完整的响应1、响应2，首先会放入队列stagedReceives中 -> 然后统一将队头响应1放在completedReceives队列中->也就是这里的处理只会获取到响应1，然后从inFlighRequests里面移除掉最先发送的请求1即可

```java
private void handleCompletedReceives(List<ClientResponse> responses, long now) {
    for (NetworkReceive receive : this.selector.completedReceives()) {
        String source = receive.source();
        //获取最先发送的请求，poll操作
        ClientRequest req = inFlightRequests.completeNext(source);
        Struct body = parseResponse(receive.payload(), req.request().header());
        //如果响应头是元数据则返回true，否则返回false，用于解析响应处理元数据的更新
        if (!metadataUpdater.maybeHandleCompletedReceive(req, now, body))
            //封装响应返回
            responses.add(new ClientResponse(req, now, false, body));
    }
}
```

## handleDisconnections处理需要断开的连接

```java
private void handleDisconnections(List<ClientResponse> responses, long now) {
    for (String node : this.selector.disconnected()) {
        log.debug("Node {} disconnected.", node);
        //修改连接状态，清空连接对应inFlightRequests队列请求
        processDisconnection(responses, node, now);
    }
    // we got a disconnect so we should probably refresh our metadata and see if that broker is dead

    //如果有连接断开，需要重新拉取元数据
    if (this.selector.disconnected().size() > 0)
        metadataUpdater.requestUpdate();
}
```

```java
private void processDisconnection(List<ClientResponse> responses, String nodeId, long now) {
    connectionStates.disconnected(nodeId, now);
    for (ClientRequest request : this.inFlightRequests.clearAll(nodeId)) {
        log.trace("Cancelled request {} due to node {} being disconnected", request, nodeId);
        if (!metadataUpdater.maybeHandleDisconnection(request))
            responses.add(new ClientResponse(request, now, true, null));
    }
}
```

## handleConnections更新连接成功的状态

```java
private void handleConnections() {
    for (String node : this.selector.connected()) {
        log.debug("Completed connection to node {}", node);
        this.connectionStates.connected(node);
    }
}
```

## handleTimedOutRequests处理超时的请求

如果发送的请求有超时，则直接断开客户端与该broker的连接

```java
private void handleTimedOutRequests(List<ClientResponse> responses, long now) {
    //判断队尾请求是否超时，查询出需要超时的节点
    List<String> nodeIds = this.inFlightRequests.getNodesWithTimedOutRequests(now, this.requestTimeoutMs);
    for (String nodeId : nodeIds) {
        // close connection to the node
        //关闭同broker的了解，释放相关资源
        this.selector.close(nodeId);
        log.debug("Disconnecting from node {} due to request timeout.", nodeId);
        //处理连接断开资源释放
        processDisconnection(responses, nodeId, now);
    }

    // we disconnected, so we should probably refresh our metadata
    if (nodeIds.size() > 0)
        metadataUpdater.requestUpdate();
}
```

关闭连接释放资源

```java
private void close(KafkaChannel channel) {
    try {
        channel.close();
    } catch (IOException e) {
        log.error("Exception closing connection to node {}:", channel.id(), e);
    }
    this.stagedReceives.remove(channel);
    this.channels.remove(channel.id());
    this.lruConnections.remove(channel.id());
    this.sensors.connectionClosed.record();
}
```

## 响应回调

在客户端发送消息，将消息封装为ClientRequest的时候，会定义好回调函数，在一次请求处理完成后，会执行该回调函数

```java
private ClientRequest produceRequest(long now, int destination, short acks, int timeout, List<RecordBatch> batches) {
    Map<TopicPartition, ByteBuffer> produceRecordsByPartition = new HashMap<TopicPartition, ByteBuffer>(batches.size());
    final Map<TopicPartition, RecordBatch> recordsByPartition = new HashMap<TopicPartition, RecordBatch>(batches.size());
    for (RecordBatch batch : batches) {
        TopicPartition tp = batch.topicPartition;
        produceRecordsByPartition.put(tp, batch.records.buffer());
        recordsByPartition.put(tp, batch);
    }
    //封装网络请求
    ProduceRequest request = new ProduceRequest(acks, timeout, produceRecordsByPartition);
    RequestSend send = new RequestSend(Integer.toString(destination),
                                       this.client.nextRequestHeader(ApiKeys.PRODUCE),
                                       request.toStruct());
    //一个ClientRequest会有对应的一个callback，网络请求响应后会执行该回调
    RequestCompletionHandler callback = new RequestCompletionHandler() {
        public void onComplete(ClientResponse response) {
            handleProduceResponse(response, recordsByPartition, time.milliseconds());
        }
    };

    return new ClientRequest(now, acks != 0, send, callback);
}
```

回调方法为handleProduceResponse

```java
private void handleProduceResponse(ClientResponse response, Map<TopicPartition, RecordBatch> batches, long now) {
    //全局唯一的，用来标识一次请求，发送请求的时候会带该参数，读取到的响应可以用来判断是对应哪一个请求
    int correlationId = response.request().request().header().correlationId();
    //如果超时或者连接断开的处理逻辑
    if (response.wasDisconnected()) {
        log.trace("Cancelled request {} due to node {} being disconnected", response, response.request()
                                                                                              .request()
                                                                                              .destination());
        //回调请求中的每个batch
        for (RecordBatch batch : batches.values())
            completeBatch(batch, Errors.NETWORK_EXCEPTION, -1L, Record.NO_TIMESTAMP, correlationId, now);
    } else {
        log.trace("Received produce response from node {} with correlation id {}",
                  response.request().request().destination(),
                  correlationId);
        // if we have a response, parse it
        //如果有响应数据的处理逻辑
        if (response.hasResponse()) {
            ProduceResponse produceResponse = new ProduceResponse(response.responseBody());
            for (Map.Entry<TopicPartition, ProduceResponse.PartitionResponse> entry : produceResponse.responses().entrySet()) {
                TopicPartition tp = entry.getKey();
                ProduceResponse.PartitionResponse partResp = entry.getValue();
                Errors error = Errors.forCode(partResp.errorCode);
                RecordBatch batch = batches.get(tp);
                completeBatch(batch, error, partResp.baseOffset, partResp.timestamp, correlationId, now);
            }
            this.sensors.recordLatency(response.request().request().destination(), response.requestLatencyMs());
            this.sensors.recordThrottleTime(response.request().request().destination(),
                                            produceResponse.getThrottleTime());
        } else {
            //不需要关注响应结果的处理
            // this is the acks = 0 case, just complete all requests
            for (RecordBatch batch : batches.values())
                completeBatch(batch, Errors.NONE, -1L, Record.NO_TIMESTAMP, correlationId, now);
        }
    }
}
```

```java
private void completeBatch(RecordBatch batch, Errors error, long baseOffset, long timestamp, long correlationId, long now) {
    //判断如果有异常，并且满足重试条件
    if (error != Errors.NONE && canRetry(batch, error)) {
        // retry
        log.warn("Got error produce response with correlation id {} on topic-partition {}, retrying ({} attempts left). Error: {}",
                 correlationId,
                 batch.topicPartition,
                 this.retries - batch.attempts - 1,
                 error);
        //如果batch可以重试，则重新加入到队头，供下次继续发送
        this.accumulator.reenqueue(batch, now);
        this.sensors.recordRetries(batch.topicPartition.topic(), batch.recordCount);
    } else {
        //重试条件不满足（可能超过最大重试次数后），也会进行回调并释放资源
        RuntimeException exception;
        if (error == Errors.TOPIC_AUTHORIZATION_FAILED)
            exception = new TopicAuthorizationException(batch.topicPartition.topic());
        else
            exception = error.exception();
        // tell the user the result of their request
        //回调batch对应的回调
        batch.done(baseOffset, timestamp, exception);
        //batch资源释放
        this.accumulator.deallocate(batch);
        if (error != Errors.NONE)
            this.sensors.recordErrors(batch.topicPartition.topic(), batch.recordCount);
    }
    if (error.exception() instanceof InvalidMetadataException)
        metadata.requestUpdate();
    // Unmute the completed partition.
    if (guaranteeMessageOrder)
        this.accumulator.unmutePartition(batch.topicPartition);
}
```

```java
public void done(long baseOffset, long timestamp, RuntimeException exception) {
        log.trace("Produced messages to topic-partition {} with base offset offset {} and error: {}.",
              topicPartition,
              baseOffset,
              exception);
    // execute callbacks
    for (int i = 0; i < this.thunks.size(); i++) {
        try {
            //回调batch中的每条消息thunk
            Thunk thunk = this.thunks.get(i);
            if (exception == null) {
                // If the timestamp returned by server is NoTimestamp, that means CreateTime is used. Otherwise LogAppendTime is used.
                RecordMetadata metadata = new RecordMetadata(this.topicPartition,  baseOffset, thunk.future.relativeOffset(),
                                                             timestamp == Record.NO_TIMESTAMP ? thunk.future.timestamp() : timestamp,
                                                             thunk.future.checksum(),
                                                             thunk.future.serializedKeySize(),
                                                             thunk.future.serializedValueSize());
                thunk.callback.onCompletion(metadata, null);
            } else {
                thunk.callback.onCompletion(null, exception);
            }
        } catch (Exception e) {
            log.error("Error executing user-provided callback on message for topic-partition {}:", topicPartition, e);
        }
    }
    //batch的结果回调
    this.produceFuture.done(topicPartition, baseOffset, exception);
}
```