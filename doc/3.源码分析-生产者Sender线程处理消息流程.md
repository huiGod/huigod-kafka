[TOC]

发送消息核心流程：

1. 根据accumulator中待发送消息对应的主题分区，检查kafka集群对应的node哪些可用，哪些不可用。得到ReadyCheckResult 结果
2. 如果ReadyCheckResult 中的unknownLeaderTopics有值，那么则需要更新Kafka集群元数据
3. 循环readyNodes，检查KafkaClient对该node是否符合网络IO的条件，不符合的从集合中删除。
4. 通过accumulator.drain()方法把待发送的消息按node号进行分组，返回Map<Integer, List<ProducerBatch>>
5. 把待发送的batch添加到Sender的inFlightBatches中。inFlightBatches是Map<TopicPartition,List<ProducerBatch>>，可见是按照主题分区来存储的。
6. 获取所有过期的batch，循环做过期处理
7. 计算接下来外层程序逻辑中调用NetWorkClient的poll操作时的timeout时间
8. 调用sendProduceRequests()方法，将待发送的ProducerBatch封装成为ClientRequest，然后“发送”出去。注意这里的发送，其实只是加入发送的队列。等到NetWorkClient进行poll操作时，才发生网络IO
9. 返回第7步中计算的poll操作timeout时间

# 初始化Sender发送消息线程

KafkaProducer客户端初始化时构造Sender源码：

```java
this.sender = new Sender(client,
                this.metadata,
                this.accumulator,
                //判断是否需要保证消息顺序
                config.getInt(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION) == 1,
                //发送 request请求最大大小max.request.size，默认1M
                config.getInt(ProducerConfig.MAX_REQUEST_SIZE_CONFIG),
                //acks
                //0：表示消息发送后就返回，不会等待任何响应机制，无法确定消息是否发送成功
                //1（默认）：表示消息往leader发送成功后，不需要等其他 follower 同步完就直接返回成功
                //all/-1：表示消息需要成功发送给leader 和所有 follower 才算成功
                (short) parseAcks(config.getString(ProducerConfig.ACKS_CONFIG)),
                //retries：默认是0不会重试
                config.getInt(ProducerConfig.RETRIES_CONFIG),
                this.metrics,
                new SystemTime(),
                clientId,
                //请求超时时间，默认30s
                this.requestTimeoutMs);
        String ioThreadName = "kafka-producer-network-thread" + (clientId.length() > 0 ? " | " + clientId : "");
        //将 sender 封装为 ioThread 线程
        //线程以及线程执行的逻辑切分开
        this.ioThread = new KafkaThread(ioThreadName, this.sender, true);
        //启动 sender 线程
        this.ioThread.start();
```

Sender源码：

```java
public class Sender implements Runnable {

    private static final Logger log = LoggerFactory.getLogger(Sender.class);

    /* the state of each nodes connection */
    private final KafkaClient client;

    /* the record accumulator that batches records */
    private final RecordAccumulator accumulator;

    /* the metadata for the client */
    private final Metadata metadata;

    /* the flag indicating whether the producer should guarantee the message order on the broker or not. */
    private final boolean guaranteeMessageOrder;

    /* the maximum request size to attempt to send to the server */
    private final int maxRequestSize;

    /* the number of acknowledgements to request from the server */
    private final short acks;

    /* the number of times to retry a failed request before giving up */
    private final int retries;

    /* the clock instance used for getting the time */
    private final Time time;

    /* true while the sender thread is still running */
    private volatile boolean running;

    /* true when the caller wants to ignore all unsent/inflight messages and force close.  */
    private volatile boolean forceClose;

    /* metrics */
    private final SenderMetrics sensors;

    /* param clientId of the client */
    private String clientId;

    /* the max time to wait for the server to respond to the request*/
    private final int requestTimeout;
```

Sender线程的run方法，会不断循环执行核心run方法：

```java
void run(long now) {
    Cluster cluster = metadata.fetch();
    // get the list of partitions with data ready to send
    //获取已经准备好可以发送 batch 的Partition Leader所在的 broker 机器节点列表
    RecordAccumulator.ReadyCheckResult result = this.accumulator.ready(cluster, now);

    // if there are any partitions whose leaders are not known yet, force metadata update
    // 如果有 leader Partition 未知，则更新元数据拉取标识
    if (result.unknownLeadersExist)
        this.metadata.requestUpdate();

    // remove any nodes we aren't ready to send to
    Iterator<Node> iter = result.readyNodes.iterator();
    long notReadyTimeout = Long.MAX_VALUE;
    while (iter.hasNext()) {
        Node node = iter.next();
        //判断 Broker连接是否准备好并且可以发送数据
        //底层逻辑会有write发送数据时的拆包判断，如果发现拆包则返回 false，不会继续发送后续 ClientRequest请求，但是此时仍然关注着OP_WRITE事件，底层网络 IO 会继续发送剩余数据
        if (!this.client.ready(node, now)) {
            //剔除未准备好要发送数据的 broker
            iter.remove();
            notReadyTimeout = Math.min(notReadyTimeout, this.client.connectionDelay(node, now));
        }
    }

    // create produce requests
    //封装需要发送的所有 batch，组装为brokerId,List<batch>
    Map<Integer, List<RecordBatch>> batches = this.accumulator.drain(cluster,
                                                                     result.readyNodes,
                                                                     this.maxRequestSize,
                                                                     now);
    //如果需要保证消息有序（max.in.flight.requests.per.connection=1）,则在batch未发送成功前禁止接受新的消息数据
    if (guaranteeMessageOrder) {
        // Mute all the partitions drained
        for (List<RecordBatch> batchList : batches.values()) {
            for (RecordBatch batch : batchList)
                this.accumulator.mutePartition(batch.topicPartition);
        }
    }

    //清除已经超时发送的 batch
    List<RecordBatch> expiredBatches = this.accumulator.abortExpiredBatches(this.requestTimeout, now);
    // update sensors
    for (RecordBatch expiredBatch : expiredBatches)
        this.sensors.recordErrors(expiredBatch.topicPartition.topic(), expiredBatch.recordCount);

    sensors.updateProduceRequestMetrics(batches);
    //将需要发送的 batch 数据针对每个 broker封装为一个ClientRequest
    List<ClientRequest> requests = createProduceRequests(batches, now);
    // If we have any nodes that are ready to send + have sendable data, poll with 0 timeout so this can immediately
    // loop and try sending more data. Otherwise, the timeout is determined by nodes that have partitions with data
    // that isn't yet sendable (e.g. lingering, backing off). Note that this specifically does not include nodes
    // with sendable data that aren't ready to send since they would cause busy looping.
    long pollTimeout = Math.min(result.nextReadyCheckDelayMs, notReadyTimeout);
    if (result.readyNodes.size() > 0) {
        log.trace("Nodes with data ready to send: {}", result.readyNodes);
        log.trace("Created {} produce requests: {}", requests.size(), requests);
        pollTimeout = 0;
    }
    //将ClientRequest绑定到broker所对应的channel网络连接，并且关注OP_WRITE事件
    for (ClientRequest request : requests)
        client.send(request, now);

    // pollTimeout时间的计算逻辑
    // if some partitions are already ready to be sent, the select time would be 0;
    // otherwise if some partition already has some data accumulated but not ready yet,
    // the select time will be the time difference between now and its linger expiry time;
    // otherwise the select time will be the time difference between now and the metadata expiry time;
    //网络 IO 读写操作
    this.client.poll(pollTimeout, now);
}
```

# 查询可以返回 batch 消息的 Leader 节点

消息在accumulator缓冲区追加，需要满足一定条件才可以发送至broker，调用的是RecordAccumulator的ready方法。返回的是准备就绪可以发送数据给分区所在节点，或者返回最早下次可以发送数据的时间，同时会返回是否有未知leader partition的标识

```java
public ReadyCheckResult ready(Cluster cluster, long nowMs) {
    //封装 broker 节点结果集，这些节点是 topic 对应的leader Partition，并且有 batch 可以发送
    //可能对于一个Broker而言，是有多个Partiton的Batch可以发送过去的，最终会去重
    Set<Node> readyNodes = new HashSet<>();
    long nextReadyCheckDelayMs = Long.MAX_VALUE;
    boolean unknownLeadersExist = false;

    //通过是否有线程通过condition阻塞等待内存释放来判断内存耗尽
    boolean exhausted = this.free.queued() > 0;
    //遍历所有的partition下的RecordBatch
    for (Map.Entry<TopicPartition, Deque<RecordBatch>> entry : this.batches.entrySet()) {
        TopicPartition part = entry.getKey();
        Deque<RecordBatch> deque = entry.getValue();

        //获取 topic + partition 对应的 leader Partition 所在节点
        Node leader = cluster.leaderFor(part);
        if (leader == null) {
            //标记有未知 leader partition存在，触发后续拉取元数据
            unknownLeadersExist = true;
        } else if (!readyNodes.contains(leader) && !muted.contains(part)) {
        		//如果需要保持消息有序，则在batch未发送成功时，禁止继续发送消息，也就是通过muted集合来判断
            //对 topic + partition 对应的 batch 队列加锁
            //这里的锁同消息写入 append 方法用的同一个锁，这里都是判断逻辑，都是轻量级操作，分段竞争锁，提升并发能力
            synchronized (deque) {
                //仅仅是获取batch 队列的队头 batch，数据的写入是在队列最后一个 batch
                //也就是仅仅只会发送所有batch 队列的第一个 batch
                RecordBatch batch = deque.peekFirst();
                if (batch != null) {
                    //判断是否重试并且达到了重试间隔时间
                    boolean backingOff = batch.attempts > 0 && batch.lastAttemptMs + retryBackoffMs > nowMs;
                    //距离上一次尝试发送已经等待的时间
                    long waitedTimeMs = nowMs - batch.lastAttemptMs;
                    //计算需要等待的时间
                    //如果是重试，则为所配置的重试间隔时间
                    //否则是所配置的最长等待发送时间linger.ms，默认是0ms，也就是每次都必须发送，这样会降低性能，无法利用在内存缓存的批处理能力
                    long timeToWaitMs = backingOff ? retryBackoffMs : lingerMs;
                    //剩余等待时间
                    long timeLeftMs = Math.max(timeToWaitMs - waitedTimeMs, 0);
                    //判断batch 队列数大于1，或者总的内存缓冲区占用空间已满
                    boolean full = deque.size() > 1 || batch.records.isFull();
                    //已经达到batch最长等待发送时间
                    boolean expired = waitedTimeMs >= timeToWaitMs;
                    //判断是否当前 batch 可以发送
                    //exhausted为true，标识内存没有空闲空间，也需要立即发送数据来释放内存
                    //closed：当前客户端要关闭掉，此时就必须立马把内存缓冲的Batch都发送出去，就是当前强制必须把所有数据都flush出去到网络里面去，此时就必须得发送
                    boolean sendable = full || expired || exhausted || closed || flushInProgress();
                    //可以发送，并且不是重试
                    if (sendable && !backingOff) {
                        //返回leader Partition 节点，因为数据的写入只会写到 leader Partition
                        readyNodes.add(leader);
                    } else {
                        // Note that this results in a conservative estimate since an un-sendable partition may have
                        // a leader that will later be found to have sendable data. However, this is good enough
                        // since we'll just wake up and then sleep again for the remaining time.
                        //如果batch 没有准备好发送，则返回下一次需要再次尝试发送的等待时间
                        //Math.min 操作返回的是所有 batch下次再重新尝试发送的最少时间
                        nextReadyCheckDelayMs = Math.min(timeLeftMs, nextReadyCheckDelayMs);
                    }
                }
            }
        }
    }

    //方法要返回的是一个复杂的数据结构，此时可以定义一些Bean
    //返回 broker 节点、下一次尝试发送 batch 的时间、存在未知leader partition 标志
    return new ReadyCheckResult(readyNodes, nextReadyCheckDelayMs, unknownLeadersExist);
}
```

# 清除未准备就绪的Leader 节点

对ready方法判断出准备好发送数据的Node信息，剔除掉未准备好真正发送数据的Node，调用的是NetworkClient的ready方法。比较重要的判断条件是，发生了拆包和已发送待响应队列满这两种特殊情况下，无法继续向该节点发送消息数据

```java
@Override
public boolean ready(Node node, long now) {
    if (node.isEmpty())
        throw new IllegalArgumentException("Cannot connect to empty node " + node);

    //判断 Broker 连接是否可以发送数据
    if (isReady(node, now))
        return true;

    //判断是否可以跟 Broker 建立连接
    if (connectionStates.canConnect(node.idString(), now))
        // if we are interested in sending to a node and we don't have a connection to it, initiate one
        //同 Broker 发起网络连接
        initiateConnect(node, now);

    return false;
}
```

判断给定的Node是否准备好发送请求

```java
@Override
public boolean isReady(Node node, long now) {
    // if we need to update our metadata now declare all requests unready to make metadata requests first
    // priority
    //判断元数据是否正在更新
    //判断Broker连接状态以及是否可以发送数据
    return !metadataUpdater.isUpdateDue(now) && canSendRequest(node.idString());
}
```

判断连接的状态，以及是否可以发送更多的请求数据

```java
private boolean canSendRequest(String node) {
    //broker 节点的网络状态是否已经连接
    //Selector上要注册很多Channel，每个Channel就代表了跟一个Broker建立的连接
    return connectionStates.isConnected(node) && selector.isChannelReady(node) && inFlightRequests.canSendMore(node);
}
```

需要额外注意的是canSendMore方法，对每个节点所发送的请求都会先进入到inFlightRequests队列中，可以通过配置项max.in.flight.requests.per.connection（默认是5）来决定队列最多可以有的请求还未得到响应

```java
public boolean canSendMore(String node) {
    Deque<ClientRequest> queue = requests.get(node);
    //inFlightRequests，有一个参数可以设置这个东西，默认是对同一个Broker同一时间最多容忍5个请求发送过去但是还没有收到响应，
    //所以如果对一个Broker已经发送了5个请求，都没收到响应，此时就不可以继续发送了
    //如果队列中队头元素存在但是没有发送完成（出现拆包），则返回 false，也就是不能再接受请求去发送，需要继续发送拆包数据
    return queue == null || queue.isEmpty() ||
           (queue.peekFirst().request().completed() && queue.size() < this.maxInFlightRequestsPerConnection);
}
```

如果连接断开且超过重试间隔时间，或者之前未建立连接则创建连接

```java
public boolean canConnect(String id, long now) {
    NodeConnectionState state = nodeState.get(id);
    if (state == null)
        //表示从未建立过连接
        return true;
    else
        //连接已经断开并且超过了重试间隔时间
        return state.state == ConnectionState.DISCONNECTED && now - state.lastConnectAttemptMs >= this.reconnectBackoffMs;
}
```

初始化网络连接调用的是NetworkClient的initiateConnect方法，最终调用的是Selector的connect方法通过NIO进行网络连接

```java
private void initiateConnect(Node node, long now) {
    String nodeConnectionId = node.idString();
    try {
        log.debug("Initiating connection to node {} at {}:{}.", node.id(), node.host(), node.port());
        //修改连接状态，状态机概念
        this.connectionStates.connecting(nodeConnectionId, now);
        //建立 broker连接
        selector.connect(nodeConnectionId,
                         new InetSocketAddress(node.host(), node.port()),
                         this.socketSendBuffer,
                         this.socketReceiveBuffer);
    } catch (IOException e) {
        /* attempt failed, we'll try again after the backoff */
        connectionStates.disconnected(nodeConnectionId, now);
        /* maybe the problem is our metadata, update it */
        metadataUpdater.requestUpdate();
        log.debug("Error connecting to node {} at {}:{}:", node.id(), node.host(), node.port(), e);
    }
}
```

# 组装发送给同一个broker的消息

需要发送的大量缓存在内存中的消息，对应的leader partition可能位于同一个node上，将这些消息通过node进行分组，能够减少网络交互次数，提升性能，通过RecordAccumulator.drain方法来处理。处理的过程是遍历节点下的所有topic+partition，看是否有足够的消息能够发送。需要注意的是每次只会发送topic+partition对应RecordBatch队列的第一个batch数据

```java
public Map<Integer, List<RecordBatch>> drain(Cluster cluster,
                                                 Set<Node> nodes,
                                             int maxSize,
                                             long now) {
    if (nodes.isEmpty())
        return Collections.emptyMap();

    Map<Integer, List<RecordBatch>> batches = new HashMap<>();
    for (Node node : nodes) {
        int size = 0;
        //获取leader所在node节点的所有topic+partition信息
        List<PartitionInfo> parts = cluster.partitionsForNode(node.id());
        List<RecordBatch> ready = new ArrayList<>();
        /* to make starvation less likely this loop doesn't start at 0 */
        int start = drainIndex = drainIndex % parts.size();
        do {
            PartitionInfo part = parts.get(drainIndex);
            TopicPartition tp = new TopicPartition(part.topic(), part.partition());
            // Only proceed if the partition has no in-flight batches.
            if (!muted.contains(tp)) {
                Deque<RecordBatch> deque = getDeque(new TopicPartition(part.topic(), part.partition()));
                if (deque != null) {
                    synchronized (deque) {
                        //查看队列中第一个 batch
                        RecordBatch first = deque.peekFirst();
                        if (first != null) {
                            boolean backoff = first.attempts > 0 && first.lastAttemptMs + retryBackoffMs > now;
                            // Only drain the batch if it is not during backoff period.
                            if (!backoff) {
                                //判断总的需要发送数据的内存大小是否超过所配置的最大内存大小
                                if (size + first.records.sizeInBytes() > maxSize && !ready.isEmpty()) {
                                    // there is a rare case that a single batch size is larger than the request size due
                                    // to compression; in this case we will still eventually send this batch in a single
                                    // request
                                    break;
                                } else {
                                    //如果当前 batch 可以发送，则从队列弹出队头batch
                                    RecordBatch batch = deque.pollFirst();
                                    //将batch底层ByteBuffer设置为只读
                                    batch.records.close();
                                    size += batch.records.sizeInBytes();
                                    ready.add(batch);
                                    batch.drainedMs = now;
                                }
                            }
                        }
                    }
                }
            }
            this.drainIndex = (this.drainIndex + 1) % parts.size();
        } while (start != drainIndex);
        batches.put(node.id(), ready);
    }
    return batches;
}
```

# 清理超时的RecordBatch数据

```java
public List<RecordBatch> abortExpiredBatches(int requestTimeout, long now) {
    List<RecordBatch> expiredBatches = new ArrayList<>();
    int count = 0;
    for (Map.Entry<TopicPartition, Deque<RecordBatch>> entry : this.batches.entrySet()) {
        Deque<RecordBatch> dq = entry.getValue();
        TopicPartition tp = entry.getKey();
        // We only check if the batch should be expired if the partition does not have a batch in flight.
        // This is to prevent later batches from being expired while an earlier batch is still in progress.
        // Note that `muted` is only ever populated if `max.in.flight.request.per.connection=1` so this protection
        // is only active in this case. Otherwise the expiration order is not guaranteed.
        if (!muted.contains(tp)) {
            synchronized (dq) {
                // iterate over the batches and expire them if they have been in the accumulator for more than requestTimeOut
                RecordBatch lastBatch = dq.peekLast();
                Iterator<RecordBatch> batchIterator = dq.iterator();
                while (batchIterator.hasNext()) {
                    RecordBatch batch = batchIterator.next();
                    boolean isFull = batch != lastBatch || batch.records.isFull();
                    // check if the batch is expired
                    //检测batch是否超时，如果超时则移除，并且释放内存空间。如果满足重试条件，该batch会继续加入到队头
                    if (batch.maybeExpire(requestTimeout, retryBackoffMs, now, this.lingerMs, isFull)) {
                        expiredBatches.add(batch);
                        count++;
                        batchIterator.remove();
                        deallocate(batch);
                    } else {
                        // Stop at the first batch that has not expired.
                        break;
                    }
                }
            }
        }
    }
    if (!expiredBatches.isEmpty())
        log.trace("Expired {} batches in accumulator", count);

    return expiredBatches;
}
```

```java
public boolean maybeExpire(int requestTimeoutMs, long retryBackoffMs, long now, long lingerMs, boolean isFull) {
    boolean expire = false;

    if (!this.inRetry() && isFull && requestTimeoutMs < (now - this.lastAppendTime))
        expire = true;
    else if (!this.inRetry() && requestTimeoutMs < (now - (this.createdMs + lingerMs)))
        expire = true;
    else if (this.inRetry() && requestTimeoutMs < (now - (this.lastAttemptMs + retryBackoffMs)))
        expire = true;

    if (expire) {
        //如果超时重试，释放该batch
        this.records.close();
        //同时进行回调，TimeoutException异常时可以进行重试
        this.done(-1L, Record.NO_TIMESTAMP, new TimeoutException("Batch containing " + recordCount + " record(s) expired due to timeout while requesting metadata from brokers for " + topicPartition));
    }

    return expire;
}
```

# 将请求数据封装为ClientRequest

对每个broker的所有请求，都进行封装为ClientRequest，供后续进行网络发送

```java
private List<ClientRequest> createProduceRequests(Map<Integer, List<RecordBatch>> collated, long now) {
    List<ClientRequest> requests = new ArrayList<ClientRequest>(collated.size());
    for (Map.Entry<Integer, List<RecordBatch>> entry : collated.entrySet())
        requests.add(produceRequest(now, entry.getKey(), acks, requestTimeout, entry.getValue()));
    return requests;
}
```

ClientRequest请求封装逻辑：

```java
private ClientRequest produceRequest(long now, int destination, short acks, int timeout, List<RecordBatch> batches) {
    Map<TopicPartition, ByteBuffer> produceRecordsByPartition = new HashMap<TopicPartition, ByteBuffer>(batches.size());
    final Map<TopicPartition, RecordBatch> recordsByPartition = new HashMap<TopicPartition, RecordBatch>(batches.size());
    for (RecordBatch batch : batches) {
        TopicPartition tp = batch.topicPartition;
        produceRecordsByPartition.put(tp, batch.records.buffer());
        recordsByPartition.put(tp, batch);
    }
    //封装网络请求
    ProduceRequest request = new ProduceRequest(acks, timeout, produceRecordsByPartition);
    RequestSend send = new RequestSend(Integer.toString(destination),
                                       this.client.nextRequestHeader(ApiKeys.PRODUCE),
                                       request.toStruct());
    //一个ClientRequest会有对应的一个callback，网络请求响应后会执行该回调
    RequestCompletionHandler callback = new RequestCompletionHandler() {
        public void onComplete(ClientResponse response) {
            handleProduceResponse(response, recordsByPartition, time.milliseconds());
        }
    };

    return new ClientRequest(now, acks != 0, send, callback);
}
```

# 消息发送绑定到KafkaChannel

1. 消息的发送需要判断，如果在InFlightRequests队列中的队头请求数据发生拆包未完成发送，或者是队列数据个数达到限制，则不会继续后续的发送流程
2. 后续的发送流程实际上是将请求ClientRequest绑定到KafkaChannel上，并且将请求数据放入到InFlightRequests对应队列中（已发送未获取响应）
3. 也就是同一时刻，KafkaChannel一定只会发送一个请求数据，待发送完成后，才能够继续发送下一个请求数据，并且才放入到InFlightRequests对应队列中

```java
@Override
public void send(ClientRequest request, long now) {
    String nodeId = request.request().destination();
    //判断节点是否准备就绪发送消息
    if (!canSendRequest(nodeId))
        throw new IllegalStateException("Attempt to send a request to node " + nodeId + " which is not ready.");
    //发送消息
    doSend(request, now);
}
```

```java
private boolean canSendRequest(String node) {
    //broker 节点的网络状态是否已经连接
    //Selector上要注册很多Channel，每个Channel就代表了跟一个Broker建立的连接
    return connectionStates.isConnected(node) && selector.isChannelReady(node) && inFlightRequests.canSendMore(node);
}
```

```java
public boolean canSendMore(String node) {
    Deque<ClientRequest> queue = requests.get(node);
    //inFlightRequests，有一个参数可以设置这个东西，默认是对同一个Broker同一时间最多容忍5个请求发送过去但是还没有收到响应，
    //所以如果对一个Broker已经发送了5个请求，都没收到响应，此时就不可以继续发送了
    //如果队列中队头元素存在但是没有发送完成（出现拆包），则返回false，也就是不能再接受请求去发送，需要继续发送拆包数据
    return queue == null || queue.isEmpty() ||
           (queue.peekFirst().request().completed() && queue.size() < this.maxInFlightRequestsPerConnection);
}
```

```java
private void doSend(ClientRequest request, long now) {
    request.setSendTimeMs(now);
    //将请求添加到inFlightRequests中的队列，并且是添加到队头，也就是最近的请求在队头
    //max.in.flight.requests.per.connection：最多允许发送给同一个 broker 的请求有多少没有返回响应
    this.inFlightRequests.add(request);
    selector.send(request.request());
}
```

```java
public void send(Send send) {
    //获取 broker 连接对应的 channel，Selector组件维护所有的channel
    KafkaChannel channel = channelOrFail(send.destination());
    try {
        //将需要发送的数据跟 channel 所绑定
        channel.setSend(send);
    } catch (CancelledKeyException e) {
        this.failedSends.add(send.destination());
        close(channel);
    }
}
```

```java
public void setSend(Send send) {
    //每个 Channel 只能同时发送一个 send 数据
    if (this.send != null)
        throw new IllegalStateException("Attempt to begin a send operation with prior send operation still in progress.");
    //赋值需要发送的数据
    this.send = send;
    //将 Selector 关注写数据（因为此时需要发送数据）
    this.transportLayer.addInterestOps(SelectionKey.OP_WRITE);
}
```

# 流程图

![kafka生产者客户端整体架构图](3.源码分析-生产者Sender线程处理消息流程.assets/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0xlb0hhbjE2Mw==,size_16,color_FFFFFF,t_70.png)

![在这里插入图片描述](3.源码分析-生产者Sender线程处理消息流程.assets/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3poYW5neGlvbmdjb2xpbg==,size_16,color_FFFFFF,t_70.png)