[TOC]

# Broker启动类入口

Kafka的Broker启动类，在Kafka.scala文件中的main方法

```java
//main 方法启动类
def main(args: Array[String]): Unit = {
try {
  val serverProps = getPropsFromArgs(args)
  val kafkaServerStartable = KafkaServerStartable.fromProps(serverProps)

  // attach shutdown handler to catch control-c
  Runtime.getRuntime().addShutdownHook(new Thread() {
    override def run() = {
      kafkaServerStartable.shutdown
    }
  })

  kafkaServerStartable.startup
  kafkaServerStartable.awaitShutdown
}
catch {
  case e: Throwable =>
    fatal(e)
    System.exit(1)
}
System.exit(0)
}
```

# Acceptor线程监听客户端请求

在KafkaServer的startup方法中，会启动SocketServer线程

```java
...
//启动SocketServer服务端
socketServer = new SocketServer(config, metrics, kafkaMetricsTime)
socketServer.startup()
...
```

Acceptor线程启动后会监听指定端口号，用于接受客户端的请求

```java
def startup() {
    this.synchronized {

      connectionQuotas = new ConnectionQuotas(maxConnectionsPerIp, maxConnectionsPerIpOverrides)

      //TCP 网络缓冲区参数
      val sendBufferSize = config.socketSendBufferBytes
      val recvBufferSize = config.socketReceiveBufferBytes
      val brokerId = config.brokerId

      var processorBeginIndex = 0
      //endpoints可以监听多个端口号，在 server.properties 配置文件中配置
      endpoints.values.foreach { endpoint =>
        val protocol = endpoint.protocolType
        //numProcessorThreads是  num.network.threads=3 线程配置项
        val processorEndIndex = processorBeginIndex + numProcessorThreads

        for (i <- processorBeginIndex until processorEndIndex)
          //初始化Processor线程
          processors(i) = newProcessor(i, connectionQuotas, protocol)

        //初始化acceptor线程
        val acceptor = new Acceptor(endpoint, sendBufferSize, recvBufferSize, brokerId,
          processors.slice(processorBeginIndex, processorEndIndex), connectionQuotas)
        acceptors.put(endpoint, acceptor)
        //启动acceptor线程
        Utils.newThread("kafka-socket-acceptor-%s-%d".format(protocol.toString, endpoint.port), acceptor, false).start()
        //阻塞等待acceptor线程启动完成
        acceptor.awaitStartup()

        processorBeginIndex = processorEndIndex
      }
    }
```

Acceptor的初始化流程，会同时启动多个processor线程

```java
private[kafka] class Acceptor(val endPoint: EndPoint,
                              val sendBufferSize: Int,
                              val recvBufferSize: Int,
                              brokerId: Int,
                              processors: Array[Processor],
                              connectionQuotas: ConnectionQuotas) extends AbstractServerThread(connectionQuotas) with KafkaMetricsGroup {

  //创建 Selector 后续用于多路复用
  private val nioSelector = NSelector.open()
  //创建ServerSocketChannel并初始化
  val serverChannel = openServerSocket(endPoint.host, endPoint.port)

  this.synchronized {
    //Acceptor在初始化时就会启动processor线程（默认3个）
    processors.foreach { processor =>
      Utils.newThread("kafka-network-thread-%d-%s-%d".format(brokerId, endPoint.protocolType.toString, processor.id), processor, false).start()
    }
  }
  ...
```

Acceptor线程本身只会处理客户端创建连接的请求，其余的读写处理会轮询交给processor线程来处理

```java
/**
   * Accept loop that checks for new connection attempts
    * 启动Acceptor线程
   */
  def run() {
    //将ServerSocketChannel注册到 Selector 并关注OP_ACCEPT事件
    serverChannel.register(nioSelector, SelectionKey.OP_ACCEPT)
    startupComplete()
    try {
      var currentProcessor = 0
      //不断循环
      while (isRunning) {
        try {
          //查看是否有 IO 操作准备就绪
          val ready = nioSelector.select(500)
          if (ready > 0) {
            val keys = nioSelector.selectedKeys()
            val iter = keys.iterator()
            while (iter.hasNext && isRunning) {
              try {
                val key = iter.next
                iter.remove()
                //如果是建立连接则处理
                if (key.isAcceptable)
                  //轮询Processor
                  accept(key, processors(currentProcessor))
                else
                  //否则抛出异常，因为 Accept 只处理创建连接的请求
                  throw new IllegalStateException("Unrecognized key state for acceptor thread.")

                // round robin to the next processor thread
                //将新的连接轮询交给processors去处理
                currentProcessor = (currentProcessor + 1) % processors.length
              } catch {
                case e: Throwable => error("Error while accepting connection", e)
              }
            }
          }
        }
        catch {
          // We catch all the throwables to prevent the acceptor thread from exiting on exceptions due
          // to a select operation on a specific channel or a bad request. We don't want the
          // the broker to stop responding to requests from other clients in these scenarios.
          case e: ControlThrowable => throw e
          case e: Throwable => error("Error occurred", e)
        }
      }
    } finally {
      debug("Closing server socket and selector.")
      swallowError(serverChannel.close())
      swallowError(nioSelector.close())
      shutdownComplete()
    }
  }
```

Acceptor处理客户端连接

```java
/*
   * Accept a new connection
   * 接受客端连接请求（OP_ACCEPT）
   */
  def accept(key: SelectionKey, processor: Processor) {
    val serverSocketChannel = key.channel().asInstanceOf[ServerSocketChannel]
    //创建socketChannel新连接
    val socketChannel = serverSocketChannel.accept()
    try {
      connectionQuotas.inc(socketChannel.socket().getInetAddress)
      //配置socketChannel参数
      socketChannel.configureBlocking(false)
      socketChannel.socket().setTcpNoDelay(true)
      socketChannel.socket().setKeepAlive(true)
      socketChannel.socket().setSendBufferSize(sendBufferSize)

      debug("Accepted connection from %s on %s and assigned it to processor %d, sendBufferSize [actual|requested]: [%d|%d] recvBufferSize [actual|requested]: [%d|%d]"
            .format(socketChannel.socket.getRemoteSocketAddress, socketChannel.socket.getLocalSocketAddress, processor.id,
                  socketChannel.socket.getSendBufferSize, sendBufferSize,
                  socketChannel.socket.getReceiveBufferSize, recvBufferSize))

      //将创建好的socketChannel连接交给processor线程处理
      processor.accept(socketChannel)
    } catch {
      case e: TooManyConnectionsException =>
        info("Rejected connection from %s, address already has the configured maximum of %d connections.".format(e.ip, e.count))
        close(socketChannel)
    }
  }
```

交给processor下的newConnections队列，供后续来处理

```java
def accept(socketChannel: SocketChannel) {
    //将客户端连接放入到Processor队列
    newConnections.add(socketChannel)
    //唤醒Processor线程对应的Selector
    wakeup()
  }
```

# Processor线程处理客户端读写

Processor初始化流程如下

```java
private[kafka] class Processor(val id: Int,
                               time: Time,
                               maxRequestSize: Int,
                               requestChannel: RequestChannel,
                               connectionQuotas: ConnectionQuotas,
                               connectionsMaxIdleMs: Long,
                               protocol: SecurityProtocol,
                               channelConfigs: java.util.Map[String, _],
                               metrics: Metrics) extends AbstractServerThread(connectionQuotas) with KafkaMetricsGroup {

  private object ConnectionId {
    def fromString(s: String): Option[ConnectionId] = s.split("-") match {
      case Array(local, remote) => BrokerEndPoint.parseHostPort(local).flatMap { case (localHost, localPort) =>
        BrokerEndPoint.parseHostPort(remote).map { case (remoteHost, remotePort) =>
          ConnectionId(localHost, localPort, remoteHost, remotePort)
        }
      }
      case _ => None
    }
  }

  //客户端连接id
  private case class ConnectionId(localHost: String, localPort: Int, remoteHost: String, remotePort: Int) {
    override def toString: String = s"$localHost:$localPort-$remoteHost:$remotePort"
  }

  //存放客户端SocketChannel连接
  private val newConnections = new ConcurrentLinkedQueue[SocketChannel]()
  //存放响应
  private val inflightResponses = mutable.Map[String, RequestChannel.Response]()
  private val metricTags = Map("networkProcessor" -> id.toString).asJava

  //对原生Selector封装
  private val selector = new KSelector(
    maxRequestSize,
    connectionsMaxIdleMs,
    metrics,
    time,
    "socket-server",
    metricTags,
    false,
    ChannelBuilders.create(protocol, Mode.SERVER, LoginType.SERVER, channelConfigs, null, true))

  	...
```

Processor线程运行run方法如下

```java
/**
* 关注事件类型的变化--精华
*/
override def run() {
startupComplete()
//循环执行
while (isRunning) {
  try {
    // setup any new connections that have been queued up
    //处理所有新创建的客户端连接，将队列中所有连接poll出注册到selector上，并且关注OP_READ事件
    configureNewConnections()
    //register any new responses for writing
    //KafkaRequestHandlerPool线程池处理完请求后，会把响应放入每个 Processor 对应的一个响应队列responseQueues里，Processor 在这里会从响应队列获取响应然后发送给客户端
    processNewResponses()
    //处理IO操作，调用 Selector 监听各个 SocketChannel 是否有请求可以进行处理
    //复用客户端使用的多路复用器Selector
    poll()
    //对已经接收完毕的请求处理
    processCompletedReceives()
    //对已发送完成的数据进行处理
    processCompletedSends()
    //处理断开的连接
    processDisconnected()
  } catch {
    // We catch all the throwables here to prevent the processor thread from exiting. We do this because
    // letting a processor exit might cause a bigger impact on the broker. Usually the exceptions thrown would
    // be either associated with a specific socket channel or a bad request. We just ignore the bad socket channel
    // or request. This behavior might need to be reviewed if we see an exception that need the entire broker to stop.
    case e: ControlThrowable => throw e
    case e: Throwable =>
      error("Processor got uncaught exception.", e)
  }
}

debug("Closing selector - processor " + id)
swallowError(closeAll())
shutdownComplete()
}
```
## configureNewConnections处理客户端新连接

客户端发起的创建连接请求，都保存在newConnections队列中

```java
/**
* Register any new connections that have been queued up
*/
private def configureNewConnections() {
//处理客户端发起的创建连接请求
while (!newConnections.isEmpty) {
  val channel = newConnections.poll()
  try {
    debug(s"Processor $id listening to new connection from ${channel.socket.getRemoteSocketAddress}")
    val localHost = channel.socket().getLocalAddress.getHostAddress
    val localPort = channel.socket().getLocalPort
    val remoteHost = channel.socket().getInetAddress.getHostAddress
    val remotePort = channel.socket().getPort
    //客户端连接id
    val connectionId = ConnectionId(localHost, localPort, remoteHost, remotePort).toString
    //创建的连接会在Map<String, KafkaChannel> channels中维护
    //注册到多路复用器上，并且关注OP_READ事件
    selector.register(connectionId, channel)
  } catch {
    // We explicitly catch all non fatal exceptions and close the socket to avoid a socket leak. The other
    // throwables will be caught in processor and logged as uncaught exceptions.
    case NonFatal(e) =>
      // need to close the channel here to avoid a socket leak.
      close(channel)
      error(s"Processor $id closed connection from ${channel.getRemoteAddress}", e)
  }
}
}
```

底层多路复用selector的register方法如下

```java
public void register(String id, SocketChannel socketChannel) throws ClosedChannelException {
    SelectionKey key = socketChannel.register(nioSelector, SelectionKey.OP_READ);
    KafkaChannel channel = channelBuilder.buildChannel(id, key, maxReceiveSize);
    key.attach(channel);
    this.channels.put(id, channel);
}
```

## processNewResponses处理响应数据

处理responseQueues响应队列中的数据，根据不同的类型来处理

```java
private def processNewResponses() {
    //获取responseQueues响应队列数据
    var curr = requestChannel.receiveResponse(id)
    while (curr != null) {
      try {
        curr.responseAction match {
          case RequestChannel.NoOpAction =>
            // There is no response to send to the client, we need to read more pipelined requests
            // that are sitting in the server's socket buffer
            curr.request.updateRequestMetrics
            trace("Socket server received empty response to send, registering for read: " + curr)
            //没有响应需要发送给客户端，因此可以关注 OP_READ 读取更多的请求数据来处理
            selector.unmute(curr.request.connectionId)
          case RequestChannel.SendAction =>
            //需要返回客户端响应，涉及到写数据
            sendResponse(curr)
          case RequestChannel.CloseConnectionAction =>
            curr.request.updateRequestMetrics
            trace("Closing socket connection actively according to the response code.")
            //关闭连接
            close(selector, curr.request.connectionId)
        }
      } finally {
      	//继续获取待发送队列的下一个数据
        curr = requestChannel.receiveResponse(id)
      }
    }
}
```

响应数据发送给客户端

```java
protected[network] def sendResponse(response: RequestChannel.Response) {
    trace(s"Socket server received response to send, registering for write and sending data: $response")
    val channel = selector.channel(response.responseSend.destination)
    // `channel` can be null if the selector closed the connection because it was idle for too long
    if (channel == null) {
      warn(s"Attempting to send response via channel for which there is no open connection, connection id $id")
      response.request.updateRequestMetrics()
    }
    else {
      //复用客户端使用的多路复用器Selector
      selector.send(response.responseSend)
      //保存发送给客户端的响应数据
      inflightResponses += (response.request.connectionId -> response)
    }
  }
```

## processCompletedReceives处理接受到的完整响应数据

可能接收到的多个请求数据，都先保存在stagedReceives队列中，每次只会取一个放到completedReceives队列中来处理，处理完后，才会继续处理下一个，保证了请求的有序性

```java
private def processCompletedReceives() {
    //遍历接收到的完整数据列表completedReceives
    selector.completedReceives.asScala.foreach { receive =>
      try {
        //获取发送数据的客户端连接
        val channel = selector.channel(receive.source)
        val session = RequestChannel.Session(new KafkaPrincipal(KafkaPrincipal.USER_TYPE, channel.principal.getName),
          channel.socketAddress)
        //组装请求对象参数
        val req = RequestChannel.Request(processor = id, connectionId = receive.source, session = session, buffer = receive.payload, startTimeMs = time.milliseconds, securityProtocol = protocol)
        //将请求线程安全地并发添加到requestQueue队列中
        requestChannel.sendRequest(req)
        //这里移除OP_READ事件，也就是同一个 KafkaChannel只会处理一个接收到的完整请求，待处理完后才能继续从completedReceives获取下一个完整请求
        selector.mute(receive.source)
      } catch {
        case e @ (_: InvalidRequestException | _: SchemaException) =>
          // note that even though we got an exception, we can assume that receive.source is valid. Issues with constructing a valid receive object were handled earlier
          error(s"Closing socket for ${receive.source} because of error", e)
          close(selector, receive.source)
      }
    }
  }
```

## processCompletedSends处理已发送完成的数据

在处理CompletedReceives的时候，移除了OP_READ 事件，在处理待发送消息后，也就是processNewResponses后，重新关注 OP_READ事件再去接收处理更多的数据

```java
private def processCompletedSends() {
    //遍历已经发送出去的数据列表
    //从inflightResponses中移除
    selector.completedSends.asScala.foreach { send =>
      val resp = inflightResponses.remove(send.destination).getOrElse {
        throw new IllegalStateException(s"Send for ${send.destination} completed, but not in `inflightResponses`")
      }
      resp.request.updateRequestMetrics()
      //再次关注 OP_READ 事件
      //处理完需要发送的数据后后，可以继续获取请求数据来处理
      selector.unmute(send.destination)
    }
  }
```

# KafkaRequestHandlerPool业务处理线程池

broker接收到的请求，是需要交给KafkaRequestHandlerPool线程池来执行具体的逻辑，在KafkaServer启动时，会对KafkaRequestHandlerPool线程池进行初始化

```java
def startup() {

	...
	/* start processing requests */
    //处理RequestChannel中请求队列的线程池
    apis = new KafkaApis(socketServer.requestChannel, replicaManager, groupCoordinator,
      kafkaController, zkUtils, config.brokerId, config, metadataCache, metrics, authorizer)
    //numIoThreads是业务处理线程，默认8
    requestHandlerPool = new KafkaRequestHandlerPool(config.brokerId, socketServer.requestChannel, apis, config.numIoThreads)
    ...
}
```

## KafkaRequestHandlerPool初始化

```java
class KafkaRequestHandlerPool(val brokerId: Int,
                              val requestChannel: RequestChannel,
                              val apis: KafkaApis,
                              numThreads: Int) extends Logging with KafkaMetricsGroup {

  /* a meter to track the average free capacity of the request handlers */
  private val aggregateIdleMeter = newMeter("RequestHandlerAvgIdlePercent", "percent", TimeUnit.NANOSECONDS)

  this.logIdent = "[Kafka Request Handler on Broker " + brokerId + "], "
  val threads = new Array[Thread](numThreads)
  val runnables = new Array[KafkaRequestHandler](numThreads)
  for(i <- 0 until numThreads) {
    runnables(i) = new KafkaRequestHandler(i, brokerId, aggregateIdleMeter, numThreads, requestChannel, apis)
    //初始化并启动线程
    threads(i) = Utils.daemonThread("kafka-request-handler-" + i, runnables(i))
    threads(i).start()
  }
  ...
}
```

## KafkaRequestHandler线程

处理接收到的具体请求逻辑

```java
class KafkaRequestHandler(id: Int,
                          brokerId: Int,
                          val aggregateIdleMeter: Meter,
                          val totalHandlerThreads: Int,
                          val requestChannel: RequestChannel,
                          apis: KafkaApis) extends Runnable with Logging {
  this.logIdent = "[Kafka Request Handler " + id + " on Broker " + brokerId + "], "

  def run() {
    //线程循环执行处理逻辑
    while(true) {
      try {
        var req : RequestChannel.Request = null
        //req为空则表示请求队列暂时为空
        while (req == null) {
          // We use a single meter for aggregate idle percentage for the thread pool.
          // Since meter is calculated as total_recorded_value / time_window and
          // time_window is independent of the number of threads, each recorded idle
          // time should be discounted by # threads.
          val startSelectTime = SystemTime.nanoseconds
          //从requestQueue队列获取请求
          req = requestChannel.receiveRequest(300)
          val idleTime = SystemTime.nanoseconds - startSelectTime
          aggregateIdleMeter.mark(idleTime / totalHandlerThreads)
        }

        if(req eq RequestChannel.AllDone) {
          debug("Kafka request handler %d on broker %d received shut down command".format(
            id, brokerId))
          return
        }
        req.requestDequeueTimeMs = SystemTime.milliseconds
        trace("Kafka request handler %d on broker %d handling request %s".format(id, brokerId, req))
        //将请求给KafkaApis处理
        apis.handle(req)
      } catch {
        case e: Throwable => error("Exception when handling request", e)
      }
    }
  }

  def shutdown(): Unit = requestChannel.sendRequest(RequestChannel.AllDone)
}
```

# 请求处理器KafkaApis

KafkaRequestHandler线程从requestQueue队列中获取到客户端发送的请求，然后通过KafkaApis来具体处理

```java
def handle(request: RequestChannel.Request) {
    try {
      trace("Handling request:%s from connection %s;securityProtocol:%s,principal:%s".
        format(request.requestDesc(true), request.connectionId, request.securityProtocol, request.session.principal))
      ApiKeys.forId(request.requestId) match {
          //生产者发送请求处理逻辑
        case ApiKeys.PRODUCE => handleProducerRequest(request)
          //处理 broker 发送的 fetchRequest 请求
        case ApiKeys.FETCH => handleFetchRequest(request)
        case ApiKeys.LIST_OFFSETS => handleOffsetRequest(request)
        case ApiKeys.METADATA => handleTopicMetadataRequest(request)
        case ApiKeys.LEADER_AND_ISR => handleLeaderAndIsrRequest(request)
        case ApiKeys.STOP_REPLICA => handleStopReplicaRequest(request)
        case ApiKeys.UPDATE_METADATA_KEY => handleUpdateMetadataRequest(request)
        case ApiKeys.CONTROLLED_SHUTDOWN_KEY => handleControlledShutdownRequest(request)
        case ApiKeys.OFFSET_COMMIT => handleOffsetCommitRequest(request)
        case ApiKeys.OFFSET_FETCH => handleOffsetFetchRequest(request)
        case ApiKeys.GROUP_COORDINATOR => handleGroupCoordinatorRequest(request)
        case ApiKeys.JOIN_GROUP => handleJoinGroupRequest(request)
        case ApiKeys.HEARTBEAT => handleHeartbeatRequest(request)
        case ApiKeys.LEAVE_GROUP => handleLeaveGroupRequest(request)
        case ApiKeys.SYNC_GROUP => handleSyncGroupRequest(request)
        case ApiKeys.DESCRIBE_GROUPS => handleDescribeGroupRequest(request)
        case ApiKeys.LIST_GROUPS => handleListGroupsRequest(request)
        case ApiKeys.SASL_HANDSHAKE => handleSaslHandshakeRequest(request)
        case ApiKeys.API_VERSIONS => handleApiVersionsRequest(request)
        case requestId => throw new KafkaException("Unknown api code " + requestId)
      }
    } catch {
      case e: Throwable =>
        if (request.requestObj != null) {
          request.requestObj.handleError(e, requestChannel, request)
          error("Error when handling request %s".format(request.requestObj), e)
        } else {
          val response = request.body.getErrorResponse(request.header.apiVersion, e)
          val respHeader = new ResponseHeader(request.header.correlationId)

          /* If request doesn't have a default error response, we just close the connection.
             For example, when produce request has acks set to 0 */
          if (response == null)
            requestChannel.closeConnection(request.processor, request)
          else
            requestChannel.sendResponse(new Response(request, new ResponseSend(request.connectionId, respHeader, response)))

          error("Error when handling request %s".format(request.body), e)
        }
    } finally
      request.apiLocalCompleteTimeMs = SystemTime.milliseconds
  }
```

# Broker 网络模型流程图

![img](5.源码分析-Broker启动流程解析.assets/2191745-20220211113513059-901466484.png)

# 流程图

![在这里插入图片描述](5.源码分析-Broker启动流程解析.assets/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5Lqr55-l,size_20,color_FFFFFF,t_70,g_se,x_16.png)